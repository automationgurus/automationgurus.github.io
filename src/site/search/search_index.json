{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome in Automation Bible Mission provide high-quality content investigate new areas be first Who are we? Artur Brodzi\u0144ski Kamil Wi\u0119cek","title":"Welcome in Automation Bible"},{"location":"#welcome-in-automation-bible","text":"","title":"Welcome in Automation Bible"},{"location":"#mission","text":"provide high-quality content investigate new areas be first","title":"Mission"},{"location":"#who-are-we","text":"","title":"Who are we?"},{"location":"#artur-brodzinski","text":"","title":"Artur Brodzi\u0144ski"},{"location":"#kamil-wiecek","text":"","title":"Kamil Wi\u0119cek"},{"location":"about-us/","text":"","title":"About us"},{"location":"automatic-certificate-generation-aks/","text":"Automatic certificate generation for Azure Kubernetes Service For all administrators which are taking care of web application support, certificates management is very annoying task. If you have reminder for that it's not a problem, however in many cases admins simply forgot to update it on time. In this post I will try to show you how to automate certificate generation and assignmnet process for Azure Kubernetes Service using certmanager , Let's Encrypt and Azure DevOps YAML pipelines about which I wrote in my previous article. If you didn't read it yet, strongly recommend to do that. Why certificate on Azure Kubernetes Service? Nowadays Kubernetes become more and more popular solution for web application. Companire are still a little bit afraid about using Platform as a Service solution in Cloud and preffer to use containers instead of it. For IT specialist containarization can be in some cases a little bit complicated, especially if they were not familiar in the past with Linux. To have secured connection between client and application stored on Azure Kubernetes Service or other container orchestrator, application must have in the front SSL certificate assigned in same way as web application stored on any other standard service. In AKS we will use Nginx Ingress for that How the process for certificate automation should looks like? Our main deployment pipeline will process in following way: 1. Helm tool installation 2. Configuration of custom namespace in AKS (In our case it will be ingress-basic) 3. Adding Nginx to repository 4. Installation of Nginx Ingress 5. Installation of custom resource of cert manager in version 0.13 6. Adding jetstack (certmanager) package to repository 7. Certmanager installation 8. Cluster issuer configuration on AKS 9. Applying Demo \"Hello world\" application 10. Certificate creation and assignment to Nginx Ingress But before main configuration let's check requirements which should be configured first. Prerequisites First off all you must have AKS cluster ready. Service connection and environment should be configured to access DevOps (described how it should be done in Multi-stage YAML pipeline post). Create public IP using below Azure CLI command - it should be created in same resource group as AKS cluster nodes. az network public-ip create --resource-group MC_myResourceGroup_myAKSCluster_eastus --name myAKSPublicIP --sku Standard --allocation-method static --query publicIp.ipAddress -o tsv Once public IP is created you can add DNS name for it directly in Azure portal. Public IP 4. Configure Variable group in Azure DevOps library with variables like on below picture (in my example it's only done for TEST environment but you can expand it as you want). Values for those variables should be taken from point 2 and 3 . 5. Configure access for Azure DevOps service connection to AKS by providing specific permission - in our case it will be cluster admin but you can limit it. Create role.yaml file as below and later deploy it using Az CLI and kubectl commands. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : cluster-admins roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : Group name : system:serviceaccounts apiGroup : rbac.authorization.k8s.io az login az account set --subscription \"SUBSCRIPTION-NAME\" az aks get-credentials --name aks-cluster --resource-group aks-resource-group kubectl -f role.yaml Now is time for real meat! Once all prerequistes are configured we can start configuration of code. Cluster issuer configuration. Our first yaml file will contain defintion of cluster issuer. In our case we will use Let's encrypt server for signing certificate. All supprted certificate issuers can be found on certmanager webpage . Create file called certificate-cluster-issuer.yaml paste content provided below and adjust your mail address. apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : your@mail.com privateKeySecretRef : name : letsencrypt solvers : - http01 : ingress : class : nginx Deploy hello world application to your AKS cluster To deploy appliaction we will use demo application from public image, but if you want you can create custom one. Copy content below and paste it to file aks-helloworld.yaml . apiVersion : apps/v1 kind : Deployment metadata : name : aks-helloworld spec : replicas : 1 selector : matchLabels : app : aks-helloworld template : metadata : labels : app : aks-helloworld spec : containers : - name : aks-helloworld image : neilpeterson/aks-helloworld:v1 ports : - containerPort : 80 env : - name : AUTOMATEGURU value : \"Welcome to AUTOMATE.GURU Azure Kubernetes Service (AKS)\" --- apiVersion : v1 kind : Service metadata : name : aks-helloworld spec : type : ClusterIP ports : - port : 80 selector : app : aks-helloworld Configure Ingress for custom domain and certificate Ingress controller needs a defintion of DNS and certificate to know exactly from which adress it should be available. As you see below our domain has been set to automateguru.westeurope.cloudapp.azure.com and it has been assigned to public IP created as a prerequiste. Certificate (secret name) is taken from tls-secret which will created in next step. As a backend we defined aks-helloworld service to point to our demo application. Below yaml should be created as ingress-assign-certificate-test.yaml file. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : hello-world-ingress annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : letsencrypt nginx.ingress.kubernetes.io/rewrite-target : /$1 spec : tls : - hosts : - automateguru.westeurope.cloudapp.azure.com secretName : tls-secret rules : - host : automateguru.westeurope.cloudapp.azure.com http : paths : - backend : serviceName : aks-helloworld servicePort : 80 path : /(.*) Certificate configuration Now we need to define configuration for our certificate. In spec section we should provide name of the secret in which certificate will be stored - it's configured in ingress-assign-certificate-test.yaml so keep it in mind that if you want to change the name it should be changed there as well. As a dnsNames we should provide public DNS name of our application. In issuerRef we are providing name of the cluster issuer which was defined in file certificate-cluster-issuer.yaml . Once yaml is adjusted we are saving it as certificate-test.yaml file. apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : tls-secret namespace : ingress-basic spec : secretName : tls-secret dnsNames : - automateguru.westeurope.cloudapp.azure.com acme : config : - http01 : ingressClass : nginx domains : - automateguru.westeurope.cloudapp.azure.com issuerRef : name : letsencrypt kind : ClusterIssuer And the magic is in the pipeline definition Once you created all yaml defintion you should create proper pipeline definition to automate whole process. Keep in mind that we are doing deployment only in test environment, so if you want to automate it for more stages you must adjust it. Configure deploy-to-all-stages.yaml file. Values of the variables are taken from Azure DevOps variable groups. stages : - template : deployment-stage.yaml parameters : STAGE_NAME : TEST STAGE_ENVIRONMENT : $(TEST_ENVIRONMENT_NAME) STAGE_K8S_SERVICE_ENDPOINT : $(TEST_CLUSTER_SERVICE_CONNECTION_NAME) STAGE_PUBLIC_IP : $(PUBLICIP_TEST) Main pipeline defintion should be stored in deployment-stage.yaml file. Whole process has been described in chapter How the process for certificate automation should looks like? parameters : STAGE_NAME : '' STAGE_ENVIRONMENT : '' STAGE_K8S_SERVICE_ENDPOINT : '' STAGE_PUBLIC_IP : '' stages : - stage : ${{ parameters.STAGE_NAME }} jobs : - deployment : SetupCluster pool : vmImage : 'ubuntu-latest' environment : ${{ parameters.STAGE_ENVIRONMENT }} strategy : runOnce : deploy : steps : - task : DownloadPipelineArtifact@2 inputs : artifactName : yaml targetPath : $(Build.SourcesDirectory)/yaml - task : HelmInstaller@1 inputs : helmVersionToInstall : '3.2.0' - task : Kubernetes@1 displayName : Configure ingress namespace inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} command : 'create' arguments : 'namespace ingress-basic' - task : Bash@3 displayName : \"Repo add nginx\" inputs : targetType : 'inline' script : 'helm repo add stable https://kubernetes-charts.storage.googleapis.com/' - task : HelmDeploy@0 displayName : Install Nginx inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'install' chartType : 'Name' chartName : 'stable/nginx-ingress' arguments : '--set controller.replicaCount=1 --set controller.service.loadBalancerIP=${{ parameters.STAGE_PUBLIC_IP }} --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux' - task : Kubernetes@1 displayName : Create custom resources inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} command : 'apply' arguments : '--validate=false -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.13/deploy/manifests/00-crds.yaml' - task : Bash@3 displayName : \"Repo add jetstack\" inputs : targetType : 'inline' script : 'helm repo add jetstack https://charts.jetstack.io' - task : Bash@3 displayName : \"Repo update\" inputs : targetType : 'inline' script : 'helm repo update' - task : HelmDeploy@0 displayName : Install cert-manager inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'install' chartType : 'Name' chartName : 'jetstack/cert-manager' arguments : '--version v0.13.0' - task : Kubernetes@1 displayName : Apply cluster issuer inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/certificate-cluster-issuer.yaml' - task : Kubernetes@1 displayName : Apply Hello World inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/aks-helloworld.yaml' - task : Kubernetes@1 displayName : Ingress assign certificate inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/ingress-assign-certificate-${{ parameters.STAGE_NAME }}.yaml' - task : Kubernetes@1 displayName : Create certificate inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/certificate-${{ parameters.STAGE_NAME }}.yaml' We are ready for deployment! If you configured everything correctly and already run your pipeline it's time to check if it's working. Go to your web browser and navigate to your application address. If you see screen similar like on above without any error probably it's working. But to be 100% sure check certificate ;) If something is not clear in this process, please leave a comment. Hope that we will be able to help you!","title":"Automatic Certificate Generation for AKS"},{"location":"automatic-certificate-generation-aks/#automatic-certificate-generation-for-azure-kubernetes-service","text":"For all administrators which are taking care of web application support, certificates management is very annoying task. If you have reminder for that it's not a problem, however in many cases admins simply forgot to update it on time. In this post I will try to show you how to automate certificate generation and assignmnet process for Azure Kubernetes Service using certmanager , Let's Encrypt and Azure DevOps YAML pipelines about which I wrote in my previous article. If you didn't read it yet, strongly recommend to do that.","title":"Automatic certificate generation for Azure Kubernetes Service"},{"location":"automatic-certificate-generation-aks/#why-certificate-on-azure-kubernetes-service","text":"Nowadays Kubernetes become more and more popular solution for web application. Companire are still a little bit afraid about using Platform as a Service solution in Cloud and preffer to use containers instead of it. For IT specialist containarization can be in some cases a little bit complicated, especially if they were not familiar in the past with Linux. To have secured connection between client and application stored on Azure Kubernetes Service or other container orchestrator, application must have in the front SSL certificate assigned in same way as web application stored on any other standard service. In AKS we will use Nginx Ingress for that","title":"Why certificate on Azure Kubernetes Service?"},{"location":"automatic-certificate-generation-aks/#how-the-process-for-certificate-automation-should-looks-like","text":"Our main deployment pipeline will process in following way: 1. Helm tool installation 2. Configuration of custom namespace in AKS (In our case it will be ingress-basic) 3. Adding Nginx to repository 4. Installation of Nginx Ingress 5. Installation of custom resource of cert manager in version 0.13 6. Adding jetstack (certmanager) package to repository 7. Certmanager installation 8. Cluster issuer configuration on AKS 9. Applying Demo \"Hello world\" application 10. Certificate creation and assignment to Nginx Ingress But before main configuration let's check requirements which should be configured first.","title":"How the process for certificate automation should looks like?"},{"location":"automatic-certificate-generation-aks/#prerequisites","text":"First off all you must have AKS cluster ready. Service connection and environment should be configured to access DevOps (described how it should be done in Multi-stage YAML pipeline post). Create public IP using below Azure CLI command - it should be created in same resource group as AKS cluster nodes. az network public-ip create --resource-group MC_myResourceGroup_myAKSCluster_eastus --name myAKSPublicIP --sku Standard --allocation-method static --query publicIp.ipAddress -o tsv Once public IP is created you can add DNS name for it directly in Azure portal. Public IP 4. Configure Variable group in Azure DevOps library with variables like on below picture (in my example it's only done for TEST environment but you can expand it as you want). Values for those variables should be taken from point 2 and 3 . 5. Configure access for Azure DevOps service connection to AKS by providing specific permission - in our case it will be cluster admin but you can limit it. Create role.yaml file as below and later deploy it using Az CLI and kubectl commands. apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : cluster-admins roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : cluster-admin subjects : - kind : Group name : system:serviceaccounts apiGroup : rbac.authorization.k8s.io az login az account set --subscription \"SUBSCRIPTION-NAME\" az aks get-credentials --name aks-cluster --resource-group aks-resource-group kubectl -f role.yaml","title":"Prerequisites"},{"location":"automatic-certificate-generation-aks/#now-is-time-for-real-meat","text":"Once all prerequistes are configured we can start configuration of code.","title":"Now is time for real meat!"},{"location":"automatic-certificate-generation-aks/#cluster-issuer-configuration","text":"Our first yaml file will contain defintion of cluster issuer. In our case we will use Let's encrypt server for signing certificate. All supprted certificate issuers can be found on certmanager webpage . Create file called certificate-cluster-issuer.yaml paste content provided below and adjust your mail address. apiVersion : cert-manager.io/v1alpha2 kind : ClusterIssuer metadata : name : letsencrypt spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : your@mail.com privateKeySecretRef : name : letsencrypt solvers : - http01 : ingress : class : nginx","title":"Cluster issuer configuration."},{"location":"automatic-certificate-generation-aks/#deploy-hello-world-application-to-your-aks-cluster","text":"To deploy appliaction we will use demo application from public image, but if you want you can create custom one. Copy content below and paste it to file aks-helloworld.yaml . apiVersion : apps/v1 kind : Deployment metadata : name : aks-helloworld spec : replicas : 1 selector : matchLabels : app : aks-helloworld template : metadata : labels : app : aks-helloworld spec : containers : - name : aks-helloworld image : neilpeterson/aks-helloworld:v1 ports : - containerPort : 80 env : - name : AUTOMATEGURU value : \"Welcome to AUTOMATE.GURU Azure Kubernetes Service (AKS)\" --- apiVersion : v1 kind : Service metadata : name : aks-helloworld spec : type : ClusterIP ports : - port : 80 selector : app : aks-helloworld","title":"Deploy hello world application to your AKS cluster"},{"location":"automatic-certificate-generation-aks/#configure-ingress-for-custom-domain-and-certificate","text":"Ingress controller needs a defintion of DNS and certificate to know exactly from which adress it should be available. As you see below our domain has been set to automateguru.westeurope.cloudapp.azure.com and it has been assigned to public IP created as a prerequiste. Certificate (secret name) is taken from tls-secret which will created in next step. As a backend we defined aks-helloworld service to point to our demo application. Below yaml should be created as ingress-assign-certificate-test.yaml file. apiVersion : extensions/v1beta1 kind : Ingress metadata : name : hello-world-ingress annotations : kubernetes.io/ingress.class : nginx cert-manager.io/cluster-issuer : letsencrypt nginx.ingress.kubernetes.io/rewrite-target : /$1 spec : tls : - hosts : - automateguru.westeurope.cloudapp.azure.com secretName : tls-secret rules : - host : automateguru.westeurope.cloudapp.azure.com http : paths : - backend : serviceName : aks-helloworld servicePort : 80 path : /(.*)","title":"Configure Ingress for custom domain and certificate"},{"location":"automatic-certificate-generation-aks/#certificate-configuration","text":"Now we need to define configuration for our certificate. In spec section we should provide name of the secret in which certificate will be stored - it's configured in ingress-assign-certificate-test.yaml so keep it in mind that if you want to change the name it should be changed there as well. As a dnsNames we should provide public DNS name of our application. In issuerRef we are providing name of the cluster issuer which was defined in file certificate-cluster-issuer.yaml . Once yaml is adjusted we are saving it as certificate-test.yaml file. apiVersion : cert-manager.io/v1alpha2 kind : Certificate metadata : name : tls-secret namespace : ingress-basic spec : secretName : tls-secret dnsNames : - automateguru.westeurope.cloudapp.azure.com acme : config : - http01 : ingressClass : nginx domains : - automateguru.westeurope.cloudapp.azure.com issuerRef : name : letsencrypt kind : ClusterIssuer","title":"Certificate configuration"},{"location":"automatic-certificate-generation-aks/#and-the-magic-is-in-the-pipeline-definition","text":"Once you created all yaml defintion you should create proper pipeline definition to automate whole process. Keep in mind that we are doing deployment only in test environment, so if you want to automate it for more stages you must adjust it. Configure deploy-to-all-stages.yaml file. Values of the variables are taken from Azure DevOps variable groups. stages : - template : deployment-stage.yaml parameters : STAGE_NAME : TEST STAGE_ENVIRONMENT : $(TEST_ENVIRONMENT_NAME) STAGE_K8S_SERVICE_ENDPOINT : $(TEST_CLUSTER_SERVICE_CONNECTION_NAME) STAGE_PUBLIC_IP : $(PUBLICIP_TEST) Main pipeline defintion should be stored in deployment-stage.yaml file. Whole process has been described in chapter How the process for certificate automation should looks like? parameters : STAGE_NAME : '' STAGE_ENVIRONMENT : '' STAGE_K8S_SERVICE_ENDPOINT : '' STAGE_PUBLIC_IP : '' stages : - stage : ${{ parameters.STAGE_NAME }} jobs : - deployment : SetupCluster pool : vmImage : 'ubuntu-latest' environment : ${{ parameters.STAGE_ENVIRONMENT }} strategy : runOnce : deploy : steps : - task : DownloadPipelineArtifact@2 inputs : artifactName : yaml targetPath : $(Build.SourcesDirectory)/yaml - task : HelmInstaller@1 inputs : helmVersionToInstall : '3.2.0' - task : Kubernetes@1 displayName : Configure ingress namespace inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} command : 'create' arguments : 'namespace ingress-basic' - task : Bash@3 displayName : \"Repo add nginx\" inputs : targetType : 'inline' script : 'helm repo add stable https://kubernetes-charts.storage.googleapis.com/' - task : HelmDeploy@0 displayName : Install Nginx inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'install' chartType : 'Name' chartName : 'stable/nginx-ingress' arguments : '--set controller.replicaCount=1 --set controller.service.loadBalancerIP=${{ parameters.STAGE_PUBLIC_IP }} --set controller.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux --set defaultBackend.nodeSelector.\"beta\\.kubernetes\\.io/os\"=linux' - task : Kubernetes@1 displayName : Create custom resources inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} command : 'apply' arguments : '--validate=false -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.13/deploy/manifests/00-crds.yaml' - task : Bash@3 displayName : \"Repo add jetstack\" inputs : targetType : 'inline' script : 'helm repo add jetstack https://charts.jetstack.io' - task : Bash@3 displayName : \"Repo update\" inputs : targetType : 'inline' script : 'helm repo update' - task : HelmDeploy@0 displayName : Install cert-manager inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'install' chartType : 'Name' chartName : 'jetstack/cert-manager' arguments : '--version v0.13.0' - task : Kubernetes@1 displayName : Apply cluster issuer inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/certificate-cluster-issuer.yaml' - task : Kubernetes@1 displayName : Apply Hello World inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/aks-helloworld.yaml' - task : Kubernetes@1 displayName : Ingress assign certificate inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/ingress-assign-certificate-${{ parameters.STAGE_NAME }}.yaml' - task : Kubernetes@1 displayName : Create certificate inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} namespace : 'ingress-basic' command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/certificate-${{ parameters.STAGE_NAME }}.yaml'","title":"And the magic is in the pipeline definition"},{"location":"automatic-certificate-generation-aks/#we-are-ready-for-deployment","text":"If you configured everything correctly and already run your pipeline it's time to check if it's working. Go to your web browser and navigate to your application address. If you see screen similar like on above without any error probably it's working. But to be 100% sure check certificate ;) If something is not clear in this process, please leave a comment. Hope that we will be able to help you!","title":"We are ready for deployment!"},{"location":"azure-devops-audit-logs-forwarding/","text":"Forward Azure DevOps Audit Logs to Log Analytics Workspace Is Log Analytics Workspace clairvoyant? No it isn't! Therefore, we developed an automated solution that is continuously streaming audit logs from Azure DevOps to Log Analytics Workspace. Let's start from demo. Modify the branch policy and start to be suspicious. Invoke synchronization manually or wait. Check the results. What is available out of the box? Browsing logs through web portal. Exporting them to JSON/ CSV Analyze them using Excel/ custom tools. More general information about Azure DevOps Auditing is available in the link below. It's not our purpose to paraphrase MS documentation. Please read the following part of their documentation to get more details about ADO auditing in general. You can find more details here! Acronyms and abbreviations ADO - Azure DevOps ORG - Azure DevOps Organization PAT - Personal Access Token from ADO AAA - Azure Automation Account ARA - Azure Run As Account AKV - Azure Key Vault LAW - Azure Log Analytics Workspace Who are you? We assume that... PS C:\\> $You.SessionLevelReadiness -GE 200 True ... so we're not providing detailed, step by step instructions on how to create every single resource required to deploy this solution. We believe that you can deploy and configure them without additional instructions, or you're able to find them on your own. Prepare infrastructure Here are required resources and it's the configuration required to deploy the described solution: Organization in Azure DevOps with enabled auditing. Personal Access Token with read audit log events, manage and delete streams scope. Azure Automation Account with Azure Run As Account . AAA string variables named KeyVaultName, WorkspaceId, OrganizationName AAA string variable named LastAzureDevOpsSyncDate with round-trip date/time pattern value (for example 2020-01-01T00:00:00.0000001Z ) Azure Key Vault AKV Get and list secret access policy for ARA. AzureDevOpsPersonalAccessToken secret in AKV containing PAT value. Azure Log Analytics Workspace. Shared key read permissions for ARA. Azure Automation Powershell SynchronizeAzureDevOpsAuditLogs Runbook ( get it here ) Solution overview. More details. Every single hour Azure Automation Runbook (AAC) is invoked by schedule. Set context. All actions are performed in the context of Azure Run As Account . This account was created during Automation Account creation. Get parameters (read details above). Get all ADO audit logs entries between LastAzureDevOpsSyncDate and current date and time. Upload event to Log Analytics Workspace via REST API call. Update LastAzureDevOpsSyncDate . Solution parameters Script Parameter Source/ Where you should set it $KeyVaultName KeyVaultName variable from Automation Account $StartTime LastAzureDevOpsSyncDate variable from Automation Account $OrganizationName OrganizationName variable from Automation Account $CustomerId WorkspaceId variable from Automation Account $PersonAccessToken AzureDevOpsPersonalAccessToken secret from Azure KeyVault $SharedKey SharedKey property from Log Analytics Workspace (Id = $CustomerId) Powershell Runbook Get SynchronizeAzureDevOpsAuditLogsRunbook Source Code We use Build-Signature and Post-LogAnalyticsData from MS DOCS: Data collector api . In this example, they use $TimeStampField variable that is global. It isn't good practice to use in function variables defined out of function scope. We replaced that. Function Build-Signature ( $customerId , $sharedKey , $date , $contentLength , $method , $contentType , $resource ) { $xHeaders = \"x-ms-date:\" + $date $stringToHash = $method + \" `n \" + $contentLength + \" `n \" + $contentType + \" `n \" + $xHeaders + \" `n \" + $resource $bytesToHash = [Text.Encoding] :: UTF8 . GetBytes ( $stringToHash ) $keyBytes = [Convert] :: FromBase64String ( $sharedKey ) $sha256 = New-Object System . Security . Cryptography . HMACSHA256 $sha256 . Key = $keyBytes $calculatedHash = $sha256 . ComputeHash ( $bytesToHash ) $encodedHash = [Convert] :: ToBase64String ( $calculatedHash ) $authorization = 'SharedKey {0}:{1}' -f $customerId , $encodedHash return $authorization } Function Post-LogAnalyticsData ( $ustomerId , $sharedKey , $body , $logType ) { $method = \"POST\" $contentType = \"application/json\" $resource = \"/api/logs\" $rfc1123date = [DateTime] :: UtcNow . ToString ( \"r\" ) $contentLength = $body . Length $signature = Build-Signature ` -customerId $customerId ` -sharedKey $sharedKey ` -date $rfc1123date ` -contentLength $contentLength ` -method $method ` -contentType $contentType ` -resource $resource $uri = \"https://\" + $customerId + \".ods.opinsights.azure.com\" + $resource + \"?api-version=2016-04-01\" $headers = @{ \"Authorization\" = $signature ; \"Log-Type\" = $logType ; \"x-ms-date\" = $rfc1123date ; \"time-generated-field\" = \"timestamp\" ; } $response = Invoke-WebRequest -Uri $uri -Method $method -ContentType $contentType -Headers $headers -Body $body -UseBasicParsing return $response . StatusCode } $LogType = \"AzureDevOps\" $Conn = Get-AutomationConnection -Name AzureRunAsConnection Connect-AzAccount -ServicePrincipal -Tenant $Conn . TenantID -ApplicationId $Conn . ApplicationID -CertificateThumbprint $Conn . CertificateThumbprint | Out-Null Select-AzSubscription -SubscriptionId $Conn . SubscriptionID | Out-Null $KeyVaultName = Get-AutomationVariable -Name KeyVaultName Write-Output -InputObject 'Get keyvault name from automation account variables - success' $OrganizationName = Get-AutomationVariable -Name OrganizationName Write-Output -InputObject 'Get Azure DevOps organization name from automation account variables - success' $CustomerId = Get-AutomationVariable -Name WorkspaceId Write-Output -InputObject 'Get Log Analytics Workspace Id from automation account variables - success' $LogAnalyticsWorkspace = Get-AzOperationalInsightsWorkspace | Where-Object -Property CustomerId -EQ $CustomerId $SharedKey = ( Get-AzOperationalInsightsWorkspaceSharedKey -ResourceGroupName $logAnalyticsWorkspace . ResourceGroupName -Name $logAnalyticsWorkspace . Name ). PrimarySharedKey Write-Output -InputObject \"Get shared key directly from ' $( $logAnalyticsWorkspace . Name ) - success\" $PersonAccessToken = ( Get-AzKeyVaultSecret -VaultName $KeyVaultName -Name 'AzureDevOpsPersonalAccessToken' ). SecretValueText Write-Output -InputObject \"Get Personal Access Token from key vault ' $( $KeyVaultName ) ' - success\" $StartTime = Get-AutomationVariable -Name LastAzureDevOpsSyncDate $StartTime = $StartTime . ToUniversalTime (). GetDateTimeFormats ( \"o\" ) [string] $EndTimeQuery = [DateTime] :: Now . ToUniversalTime (). GetDateTimeFormats ( \"o\" ) Write-Output -InputObject \"Script will look for audi events created between $( $StartTime ) and $( $endTimeQuery ) \" $Base64AuthInfo = [Convert] :: ToBase64String ( [Text.Encoding] :: ASCII . GetBytes (( \"{0}:{1}\" -f 'basic' , $PersonAccessToken ))) $Headers = @{ Authorization = \"Basic $Base64AuthInfo\" } [array] $ApiOutputs = @() [string] $ContinuationToken = '' do { $EndpointUri = \"https://auditservice.dev.azure.com/ $( $OrganizationName ) /_apis/audit/auditlog?api-version=5.1-preview.1\" $EndpointUri += \"&batchSize=200\" $EndpointUri += \"&skipAggregation=true\" $EndpointUri += \"&startTime= $( $StartTime ) \" $EndpointUri += \"&endTime= $( $endTimeQuery ) \" if ( $ContinuationToken ) { $EndpointUri += \"&continuationToken= $( $continuationToken ) \" } $ApiOutput = Invoke-RestMethod -Uri $endpointUri -Headers $headers -Method Get $ContinuationToken = $ApiOutput . continuationToken #tu $ApiOutputs += $ApiOutput } while ( $ApiOutput . hasMore ) [array] $DecoratedAuditLogEntries = $ApiOutputs . decoratedAuditLogEntries if ( -not $DecoratedAuditLogEntries ) { Write-Output -InputObject 'There are no new audit logs.' return ; } Write-Output -InputObject \"Found $( $DecoratedAuditLogEntries . Count ) new audit entries\" foreach ( $item in $DecoratedAuditLogEntries ) { $item . data = $item . data | ConvertTo-Json -Compress -Depth 100 } $RecordsJson = $DecoratedAuditLogEntries | ` Select-Object -ExcludeProperty actorImageUrl | ` ConvertTo-Json $StatusCode = Post-LogAnalyticsData -customerId $CustomerId -sharedKey $SharedKey -body ( [System.Text.Encoding] :: UTF8 . GetBytes ( $recordsJson )) -logType $LogType if ( $StatusCode -eq 200 ){ Set-AutomationVariable -Name LastAzureDevOpsSyncDate -Value $endTimeQuery Write-Output -InputObject 'Azure DevOps audi logs forwarding completed successfully' } It's time to rest and check what we did You've just discovered new extension installed. You wonder who and when did install this AzSK Extension, so you ask Log Analytics. AzureDevOps_CL | sort by TimeGenerated desc nulls last | where actionId_s == \"Extension.Installed\" | project TimeGenerated, actionId_s, scopeDisplayName_s , details_s, actorDisplayName_s TimeGenerated actionId_s scopeDisplayName_s details_s actorDisplayName_s 2020-03-15T16:47:19.367Z Extension.Installed AutomationGuyIO (Organization) Extension \"Secure DevOps Kit (AzSK) CICD Extensions for Azure\" from publisher \"Microsoft DevLabs\" was installed - Version \"3.1.7\" Automation Guru Solution development insights ARA becomes a Contributor by default. Consider changing that. Storing LAW SharedKey in AKV is one of the options, but it'll force you to update it on change. We decided to get it directly during the script execution. We could use AAA encrypted value to store PAT, but in case of storing secrets, AKV should always be the primary choice. Other parameters we don't consider as secrets, so we store them in AAA variables. Enabling Allow trusted Microsoft services to bypass this firewall in AKW Networking configuration didn't allow access from AAA. Therefore we set this setting to Allow access from all networks . Visit also Dominic Batstone's Blog: Export ADO Audit Logs and query them with LogParser mohitgoyal.co: Working with Audit logs in Azure DevOps","title":"Azure DevOps Audit Logs Forwarder"},{"location":"azure-devops-audit-logs-forwarding/#forward-azure-devops-audit-logs-to-log-analytics-workspace","text":"","title":"Forward Azure DevOps Audit Logs to Log Analytics Workspace"},{"location":"azure-devops-audit-logs-forwarding/#is-log-analytics-workspace-clairvoyant","text":"No it isn't! Therefore, we developed an automated solution that is continuously streaming audit logs from Azure DevOps to Log Analytics Workspace.","title":"Is Log Analytics Workspace clairvoyant?"},{"location":"azure-devops-audit-logs-forwarding/#lets-start-from-demo","text":"Modify the branch policy and start to be suspicious. Invoke synchronization manually or wait. Check the results.","title":"Let's start from demo."},{"location":"azure-devops-audit-logs-forwarding/#what-is-available-out-of-the-box","text":"Browsing logs through web portal. Exporting them to JSON/ CSV Analyze them using Excel/ custom tools. More general information about Azure DevOps Auditing is available in the link below. It's not our purpose to paraphrase MS documentation. Please read the following part of their documentation to get more details about ADO auditing in general. You can find more details here!","title":"What is available out of the box?"},{"location":"azure-devops-audit-logs-forwarding/#acronyms-and-abbreviations","text":"ADO - Azure DevOps ORG - Azure DevOps Organization PAT - Personal Access Token from ADO AAA - Azure Automation Account ARA - Azure Run As Account AKV - Azure Key Vault LAW - Azure Log Analytics Workspace","title":"Acronyms and abbreviations"},{"location":"azure-devops-audit-logs-forwarding/#who-are-you","text":"We assume that... PS C:\\> $You.SessionLevelReadiness -GE 200 True ... so we're not providing detailed, step by step instructions on how to create every single resource required to deploy this solution. We believe that you can deploy and configure them without additional instructions, or you're able to find them on your own.","title":"Who are you?"},{"location":"azure-devops-audit-logs-forwarding/#prepare-infrastructure","text":"Here are required resources and it's the configuration required to deploy the described solution: Organization in Azure DevOps with enabled auditing. Personal Access Token with read audit log events, manage and delete streams scope. Azure Automation Account with Azure Run As Account . AAA string variables named KeyVaultName, WorkspaceId, OrganizationName AAA string variable named LastAzureDevOpsSyncDate with round-trip date/time pattern value (for example 2020-01-01T00:00:00.0000001Z ) Azure Key Vault AKV Get and list secret access policy for ARA. AzureDevOpsPersonalAccessToken secret in AKV containing PAT value. Azure Log Analytics Workspace. Shared key read permissions for ARA. Azure Automation Powershell SynchronizeAzureDevOpsAuditLogs Runbook ( get it here )","title":"Prepare infrastructure"},{"location":"azure-devops-audit-logs-forwarding/#solution-overview-more-details","text":"Every single hour Azure Automation Runbook (AAC) is invoked by schedule. Set context. All actions are performed in the context of Azure Run As Account . This account was created during Automation Account creation. Get parameters (read details above). Get all ADO audit logs entries between LastAzureDevOpsSyncDate and current date and time. Upload event to Log Analytics Workspace via REST API call. Update LastAzureDevOpsSyncDate .","title":"Solution overview. More details."},{"location":"azure-devops-audit-logs-forwarding/#solution-parameters","text":"Script Parameter Source/ Where you should set it $KeyVaultName KeyVaultName variable from Automation Account $StartTime LastAzureDevOpsSyncDate variable from Automation Account $OrganizationName OrganizationName variable from Automation Account $CustomerId WorkspaceId variable from Automation Account $PersonAccessToken AzureDevOpsPersonalAccessToken secret from Azure KeyVault $SharedKey SharedKey property from Log Analytics Workspace (Id = $CustomerId)","title":"Solution parameters"},{"location":"azure-devops-audit-logs-forwarding/#powershell-runbook","text":"Get SynchronizeAzureDevOpsAuditLogsRunbook Source Code We use Build-Signature and Post-LogAnalyticsData from MS DOCS: Data collector api . In this example, they use $TimeStampField variable that is global. It isn't good practice to use in function variables defined out of function scope. We replaced that. Function Build-Signature ( $customerId , $sharedKey , $date , $contentLength , $method , $contentType , $resource ) { $xHeaders = \"x-ms-date:\" + $date $stringToHash = $method + \" `n \" + $contentLength + \" `n \" + $contentType + \" `n \" + $xHeaders + \" `n \" + $resource $bytesToHash = [Text.Encoding] :: UTF8 . GetBytes ( $stringToHash ) $keyBytes = [Convert] :: FromBase64String ( $sharedKey ) $sha256 = New-Object System . Security . Cryptography . HMACSHA256 $sha256 . Key = $keyBytes $calculatedHash = $sha256 . ComputeHash ( $bytesToHash ) $encodedHash = [Convert] :: ToBase64String ( $calculatedHash ) $authorization = 'SharedKey {0}:{1}' -f $customerId , $encodedHash return $authorization } Function Post-LogAnalyticsData ( $ustomerId , $sharedKey , $body , $logType ) { $method = \"POST\" $contentType = \"application/json\" $resource = \"/api/logs\" $rfc1123date = [DateTime] :: UtcNow . ToString ( \"r\" ) $contentLength = $body . Length $signature = Build-Signature ` -customerId $customerId ` -sharedKey $sharedKey ` -date $rfc1123date ` -contentLength $contentLength ` -method $method ` -contentType $contentType ` -resource $resource $uri = \"https://\" + $customerId + \".ods.opinsights.azure.com\" + $resource + \"?api-version=2016-04-01\" $headers = @{ \"Authorization\" = $signature ; \"Log-Type\" = $logType ; \"x-ms-date\" = $rfc1123date ; \"time-generated-field\" = \"timestamp\" ; } $response = Invoke-WebRequest -Uri $uri -Method $method -ContentType $contentType -Headers $headers -Body $body -UseBasicParsing return $response . StatusCode } $LogType = \"AzureDevOps\" $Conn = Get-AutomationConnection -Name AzureRunAsConnection Connect-AzAccount -ServicePrincipal -Tenant $Conn . TenantID -ApplicationId $Conn . ApplicationID -CertificateThumbprint $Conn . CertificateThumbprint | Out-Null Select-AzSubscription -SubscriptionId $Conn . SubscriptionID | Out-Null $KeyVaultName = Get-AutomationVariable -Name KeyVaultName Write-Output -InputObject 'Get keyvault name from automation account variables - success' $OrganizationName = Get-AutomationVariable -Name OrganizationName Write-Output -InputObject 'Get Azure DevOps organization name from automation account variables - success' $CustomerId = Get-AutomationVariable -Name WorkspaceId Write-Output -InputObject 'Get Log Analytics Workspace Id from automation account variables - success' $LogAnalyticsWorkspace = Get-AzOperationalInsightsWorkspace | Where-Object -Property CustomerId -EQ $CustomerId $SharedKey = ( Get-AzOperationalInsightsWorkspaceSharedKey -ResourceGroupName $logAnalyticsWorkspace . ResourceGroupName -Name $logAnalyticsWorkspace . Name ). PrimarySharedKey Write-Output -InputObject \"Get shared key directly from ' $( $logAnalyticsWorkspace . Name ) - success\" $PersonAccessToken = ( Get-AzKeyVaultSecret -VaultName $KeyVaultName -Name 'AzureDevOpsPersonalAccessToken' ). SecretValueText Write-Output -InputObject \"Get Personal Access Token from key vault ' $( $KeyVaultName ) ' - success\" $StartTime = Get-AutomationVariable -Name LastAzureDevOpsSyncDate $StartTime = $StartTime . ToUniversalTime (). GetDateTimeFormats ( \"o\" ) [string] $EndTimeQuery = [DateTime] :: Now . ToUniversalTime (). GetDateTimeFormats ( \"o\" ) Write-Output -InputObject \"Script will look for audi events created between $( $StartTime ) and $( $endTimeQuery ) \" $Base64AuthInfo = [Convert] :: ToBase64String ( [Text.Encoding] :: ASCII . GetBytes (( \"{0}:{1}\" -f 'basic' , $PersonAccessToken ))) $Headers = @{ Authorization = \"Basic $Base64AuthInfo\" } [array] $ApiOutputs = @() [string] $ContinuationToken = '' do { $EndpointUri = \"https://auditservice.dev.azure.com/ $( $OrganizationName ) /_apis/audit/auditlog?api-version=5.1-preview.1\" $EndpointUri += \"&batchSize=200\" $EndpointUri += \"&skipAggregation=true\" $EndpointUri += \"&startTime= $( $StartTime ) \" $EndpointUri += \"&endTime= $( $endTimeQuery ) \" if ( $ContinuationToken ) { $EndpointUri += \"&continuationToken= $( $continuationToken ) \" } $ApiOutput = Invoke-RestMethod -Uri $endpointUri -Headers $headers -Method Get $ContinuationToken = $ApiOutput . continuationToken #tu $ApiOutputs += $ApiOutput } while ( $ApiOutput . hasMore ) [array] $DecoratedAuditLogEntries = $ApiOutputs . decoratedAuditLogEntries if ( -not $DecoratedAuditLogEntries ) { Write-Output -InputObject 'There are no new audit logs.' return ; } Write-Output -InputObject \"Found $( $DecoratedAuditLogEntries . Count ) new audit entries\" foreach ( $item in $DecoratedAuditLogEntries ) { $item . data = $item . data | ConvertTo-Json -Compress -Depth 100 } $RecordsJson = $DecoratedAuditLogEntries | ` Select-Object -ExcludeProperty actorImageUrl | ` ConvertTo-Json $StatusCode = Post-LogAnalyticsData -customerId $CustomerId -sharedKey $SharedKey -body ( [System.Text.Encoding] :: UTF8 . GetBytes ( $recordsJson )) -logType $LogType if ( $StatusCode -eq 200 ){ Set-AutomationVariable -Name LastAzureDevOpsSyncDate -Value $endTimeQuery Write-Output -InputObject 'Azure DevOps audi logs forwarding completed successfully' }","title":"Powershell Runbook"},{"location":"azure-devops-audit-logs-forwarding/#its-time-to-rest-and-check-what-we-did","text":"You've just discovered new extension installed. You wonder who and when did install this AzSK Extension, so you ask Log Analytics. AzureDevOps_CL | sort by TimeGenerated desc nulls last | where actionId_s == \"Extension.Installed\" | project TimeGenerated, actionId_s, scopeDisplayName_s , details_s, actorDisplayName_s TimeGenerated actionId_s scopeDisplayName_s details_s actorDisplayName_s 2020-03-15T16:47:19.367Z Extension.Installed AutomationGuyIO (Organization) Extension \"Secure DevOps Kit (AzSK) CICD Extensions for Azure\" from publisher \"Microsoft DevLabs\" was installed - Version \"3.1.7\" Automation Guru","title":"It's time to rest and check what we did"},{"location":"azure-devops-audit-logs-forwarding/#solution-development-insights","text":"ARA becomes a Contributor by default. Consider changing that. Storing LAW SharedKey in AKV is one of the options, but it'll force you to update it on change. We decided to get it directly during the script execution. We could use AAA encrypted value to store PAT, but in case of storing secrets, AKV should always be the primary choice. Other parameters we don't consider as secrets, so we store them in AAA variables. Enabling Allow trusted Microsoft services to bypass this firewall in AKW Networking configuration didn't allow access from AAA. Therefore we set this setting to Allow access from all networks .","title":"Solution development insights"},{"location":"azure-devops-audit-logs-forwarding/#visit-also","text":"Dominic Batstone's Blog: Export ADO Audit Logs and query them with LogParser mohitgoyal.co: Working with Audit logs in Azure DevOps","title":"Visit also"},{"location":"azure-lighthouse-configuration/","text":"Azure Lighthouse configuration Why should you read this article? What are the benefits? You want to manage all your customer in one place. You don't want to switch between subscriptions or between accounts. You want to manage access to specific customer roles without asking each time customer to do that. You are awesome managed service provider who is using best services in Azure in order to do that. High Level overview From Service provider team perspective Service provider tenant user which is added to proper group get access to My Cusotmers tab in Azure Lighthouse Can see all customers which are assigned to him. Can check AAD Group Membership and related role at the customer destination subscription. Can check access to customer resource From customer perspective Customer get access to new tab in Azure Lighthouse Service Providers Can see all service providers Display exact delegations for each provider Check offer details and correlated assigned role Prerequisites Az PowerShell Module installed Account with Owner role in customer tenant Access to create and configure Azure Active Directory groups in Manged Service Provider tenant Service Provider ARM template To deploy desired configuration in customer tenant below template must be used. In such ARM template we basically do the mapping between groups created earlier in Azure AD of Managed Service Provider tenant and role IDs defined in customer tenant. To check role defintion ID use Powershell command ( Get-AzRoleDefinition -Name \"Name_of_the_role\" ). Id To check Azure Active Directory group ID use Powershell command ( Get-AzADGroup -Name \"Name_of_the_AAD_group\" ). ObjectId If you don't have yet groups created you can simply create them with following script: param ( $TenantID , $CustomerName ) Connect-AzAccount -Tenant $TenantID $groups = @( \"Logs-Reader\" , \"Customer-Reader\" , \"Customer-Contributor\" ) foreach ( $group in $groups ) { $groupName = \"Lighthouse-$CustomerName-$group\" $aadGroup = Get-AzADGroup -DisplayName $groupName if ( $aadGroup ) { Write-Output \"Group $groupName already exists.\" } else { Try { New-AzADGroup -DisplayName $groupName -MailNickname $groupName Start-Sleep -s 5 $aadGroup = ( Get-AzADGroup -DisplayName $groupName ). ObjectId Write-Output \"AAD Group $groupName with id $aadGroupId created with success\" } Catch { Write-Output \"Unexpected error occured during AAD group creation. Error: $( $_ . exception . Message ) \" } } } Here you can find short description of each parameter in ARM: mspOfferName - name of the offer from the Managed Service Provider mspOfferDescription - name of the Managed Service Provider offering managedByTenantId - Managed Service Provider tenant ID authorizations - in this part you should provide array according to how you want to configure access in Azure Lighthouse. \"parameters\": { \"mspOfferName\": { \"value\": \"\" }, \"mspOfferDescription\": { \"value\": \"\" }, \"managedByTenantId\": { \"value\": \"\" }, \"authorizations\": { \"value\": [ { \"principalId\": \"000000-0000-00000-0000-0000000\", \"principalIdDisplayName\": \"Logs Reader\", \"roleDefinitionId\": \"73c42c96-874c-492b-b04d-ab87d138a893\" }, { \"principalId\": \"000000-0000-00000-0000-0000000\", \"principalIdDisplayName\": \"Customer Reader\", \"roleDefinitionId\": \"acdd72a7-3385-48ef-bd42-f606fba81ae7\" }, { \"principalId\": \"000000-0000-00000-0000-0000000\", \"principalIdDisplayName\": \"Customer Contributor\", \"roleDefinitionId\": \"b24988ac-6180-42a0-ab88-20f7382dd24c\" } ] } } In order to deploy such ARM you can simply use New-AzDeployment from PowerShell Az module. IMPORTANT! Remember that you must be logged to customer tenant to deploy ARM template, not Managed Service Provider tenant. Things to have in mind before applying this solution Decide how you want to manage access to your customers. Do you want to have same groups for all of them or separate one? Decide which roles are necessary for you and split them correctly - you can always do it later however it's best to have solution ready from the beginning.","title":"Azure Lighthouse Configuration"},{"location":"azure-lighthouse-configuration/#azure-lighthouse-configuration","text":"","title":"Azure Lighthouse configuration"},{"location":"azure-lighthouse-configuration/#why-should-you-read-this-article-what-are-the-benefits","text":"You want to manage all your customer in one place. You don't want to switch between subscriptions or between accounts. You want to manage access to specific customer roles without asking each time customer to do that. You are awesome managed service provider who is using best services in Azure in order to do that.","title":"Why should you read this article? What are the benefits?"},{"location":"azure-lighthouse-configuration/#high-level-overview","text":"","title":"High Level overview"},{"location":"azure-lighthouse-configuration/#from-service-provider-team-perspective","text":"Service provider tenant user which is added to proper group get access to My Cusotmers tab in Azure Lighthouse Can see all customers which are assigned to him. Can check AAD Group Membership and related role at the customer destination subscription. Can check access to customer resource","title":"From Service provider team perspective"},{"location":"azure-lighthouse-configuration/#from-customer-perspective","text":"Customer get access to new tab in Azure Lighthouse Service Providers Can see all service providers Display exact delegations for each provider Check offer details and correlated assigned role","title":"From customer perspective"},{"location":"azure-lighthouse-configuration/#prerequisites","text":"Az PowerShell Module installed Account with Owner role in customer tenant Access to create and configure Azure Active Directory groups in Manged Service Provider tenant","title":"Prerequisites"},{"location":"azure-lighthouse-configuration/#service-provider-arm-template","text":"To deploy desired configuration in customer tenant below template must be used. In such ARM template we basically do the mapping between groups created earlier in Azure AD of Managed Service Provider tenant and role IDs defined in customer tenant. To check role defintion ID use Powershell command ( Get-AzRoleDefinition -Name \"Name_of_the_role\" ). Id To check Azure Active Directory group ID use Powershell command ( Get-AzADGroup -Name \"Name_of_the_AAD_group\" ). ObjectId If you don't have yet groups created you can simply create them with following script: param ( $TenantID , $CustomerName ) Connect-AzAccount -Tenant $TenantID $groups = @( \"Logs-Reader\" , \"Customer-Reader\" , \"Customer-Contributor\" ) foreach ( $group in $groups ) { $groupName = \"Lighthouse-$CustomerName-$group\" $aadGroup = Get-AzADGroup -DisplayName $groupName if ( $aadGroup ) { Write-Output \"Group $groupName already exists.\" } else { Try { New-AzADGroup -DisplayName $groupName -MailNickname $groupName Start-Sleep -s 5 $aadGroup = ( Get-AzADGroup -DisplayName $groupName ). ObjectId Write-Output \"AAD Group $groupName with id $aadGroupId created with success\" } Catch { Write-Output \"Unexpected error occured during AAD group creation. Error: $( $_ . exception . Message ) \" } } } Here you can find short description of each parameter in ARM: mspOfferName - name of the offer from the Managed Service Provider mspOfferDescription - name of the Managed Service Provider offering managedByTenantId - Managed Service Provider tenant ID authorizations - in this part you should provide array according to how you want to configure access in Azure Lighthouse. \"parameters\": { \"mspOfferName\": { \"value\": \"\" }, \"mspOfferDescription\": { \"value\": \"\" }, \"managedByTenantId\": { \"value\": \"\" }, \"authorizations\": { \"value\": [ { \"principalId\": \"000000-0000-00000-0000-0000000\", \"principalIdDisplayName\": \"Logs Reader\", \"roleDefinitionId\": \"73c42c96-874c-492b-b04d-ab87d138a893\" }, { \"principalId\": \"000000-0000-00000-0000-0000000\", \"principalIdDisplayName\": \"Customer Reader\", \"roleDefinitionId\": \"acdd72a7-3385-48ef-bd42-f606fba81ae7\" }, { \"principalId\": \"000000-0000-00000-0000-0000000\", \"principalIdDisplayName\": \"Customer Contributor\", \"roleDefinitionId\": \"b24988ac-6180-42a0-ab88-20f7382dd24c\" } ] } } In order to deploy such ARM you can simply use New-AzDeployment from PowerShell Az module. IMPORTANT! Remember that you must be logged to customer tenant to deploy ARM template, not Managed Service Provider tenant.","title":"Service Provider ARM template"},{"location":"azure-lighthouse-configuration/#things-to-have-in-mind-before-applying-this-solution","text":"Decide how you want to manage access to your customers. Do you want to have same groups for all of them or separate one? Decide which roles are necessary for you and split them correctly - you can always do it later however it's best to have solution ready from the beginning.","title":"Things to have in mind before applying this solution"},{"location":"azure-policy-continous-integration/","text":"Security and Compliance as a Code: Azure Policy Continous Inspection In this blog entry, we will show you how to integrate automated Azure Policy tests with your DevOps process . Automated Azure Policy deployment, invoking automated Pester test, and publish tests report will be part of your pull request process. These steps will be performed in the sandbox so that it will be safe for the rest of your environment. After completing the steps described in this and last article, you will have everything ready to start writing Azure Policy without the need to perform many manual tests. The most important question always is why! In one of the previous articles , we presented Pester as a tool used for test-driven development of Azure Policy. You might find it overkill to achieve test-driven development for Azure Policy . We agree that usually , it will not be required to write tests. You will also find out scenario details there. The current state of the art is that we have some complex requirements regarding tag values. We have to validate tag values ( emails , dates , multiple values separated by a slash ). It is not that easy to satisfy them without using ARM function logics inside Azure Policy . The next part of this short Compliance as a Code series will be about more advanced techniques related to Azure Policy development. Then you will see how complex the implementation can be! Reading this article is a must for you... ... if at least one of the statement is true: You already use Azure Policy, or you are going to use it. Having a consistent and compliant Azure environment makes sense for you. An automated way of validating your changes is welcome for you. You want to perform end-to-end Azure Policy assignment effects checks without any additional effort. You are sick when it comes to performing manual work. How will this process look like finally? There is a new compliance requirement , or one of the currents has changed. A new feature branch is created for Azure Policy development. ARM template with Azure Policy/ Policy Set, Initiative, and Assignment resources is ready. A new Pull Request is created. A pipeline that deploys policy to a dedicated subscription is triggered . It waits 30 minutes to be sure that assignments are in-place. It validates if the policy is working as expected by performing automated Pester tests . Test results are generated and published. PR cannot be completed if any test has failed. PR cannot be completed if any test has failed. Who are you? We assume that... PS C : \\> $You . SessionLevelReadiness -GE 200 True ... so we are re not providing detailed, step by step instructions on how to create every single resource required to deploy this solution. We believe that you can deploy and configure them without additional instructions, or you are able to find them on your own. Almost step by step guide. Repository hierarchy. \u00a6 \u00a6 README.md \u00a6 +---arm \u00a6 ResourceGroups.json \u00a6 TaggingGovernance.json \u00a6 +---scripts Tagging.Tests.ps1 TaggingGovernance.json is a placeholder for Azure Policy implementation. { \"$schema\" : \"https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#\" , \"contentVersion\" : \"1.0.0.0\" , \"parameters\" : {}, \"resources\" : [] } Pipeline Pipeline details cleanup current azure policy assignment - script Get-AzPolicyAssignment | ` Where-Object -Property Name -NE 'SecurityCenterBuiltIn' | ` Remove-AzPolicyAssignment install pester - script Install-Module -Name Pester -Force -SkipPublisherCheck run tests - script Set-Location -Path $Env:BUILD_SOURCESDIRECTORY Invoke-Pester -OutputFile \"./testresult.xml\" -OutputFormat 'NUnitXML' publish test result It should like similar: Branch policy","title":"Continous Inspection For Azure Policy"},{"location":"azure-policy-continous-integration/#security-and-compliance-as-a-code-azure-policy-continous-inspection","text":"In this blog entry, we will show you how to integrate automated Azure Policy tests with your DevOps process . Automated Azure Policy deployment, invoking automated Pester test, and publish tests report will be part of your pull request process. These steps will be performed in the sandbox so that it will be safe for the rest of your environment. After completing the steps described in this and last article, you will have everything ready to start writing Azure Policy without the need to perform many manual tests.","title":"Security and Compliance as a Code: Azure Policy Continous Inspection"},{"location":"azure-policy-continous-integration/#the-most-important-question-always-is-why","text":"In one of the previous articles , we presented Pester as a tool used for test-driven development of Azure Policy. You might find it overkill to achieve test-driven development for Azure Policy . We agree that usually , it will not be required to write tests. You will also find out scenario details there. The current state of the art is that we have some complex requirements regarding tag values. We have to validate tag values ( emails , dates , multiple values separated by a slash ). It is not that easy to satisfy them without using ARM function logics inside Azure Policy .","title":"The most important question always is why!"},{"location":"azure-policy-continous-integration/#the-next-part-of-this-short-compliance-as-a-code-series-will-be-about-more-advanced-techniques-related-to-azure-policy-development","text":"Then you will see how complex the implementation can be!","title":"The next part of this short Compliance as a Code series will be about more advanced techniques related to Azure Policy development."},{"location":"azure-policy-continous-integration/#reading-this-article-is-a-must-for-you","text":"... if at least one of the statement is true: You already use Azure Policy, or you are going to use it. Having a consistent and compliant Azure environment makes sense for you. An automated way of validating your changes is welcome for you. You want to perform end-to-end Azure Policy assignment effects checks without any additional effort. You are sick when it comes to performing manual work.","title":"Reading this article is a must for you..."},{"location":"azure-policy-continous-integration/#how-will-this-process-look-like-finally","text":"There is a new compliance requirement , or one of the currents has changed. A new feature branch is created for Azure Policy development. ARM template with Azure Policy/ Policy Set, Initiative, and Assignment resources is ready. A new Pull Request is created. A pipeline that deploys policy to a dedicated subscription is triggered . It waits 30 minutes to be sure that assignments are in-place. It validates if the policy is working as expected by performing automated Pester tests . Test results are generated and published. PR cannot be completed if any test has failed.","title":"How will this process look like finally?"},{"location":"azure-policy-continous-integration/#pr-cannot-be-completed-if-any-test-has-failed","text":"","title":"PR cannot be completed if any test has failed."},{"location":"azure-policy-continous-integration/#who-are-you","text":"We assume that... PS C : \\> $You . SessionLevelReadiness -GE 200 True ... so we are re not providing detailed, step by step instructions on how to create every single resource required to deploy this solution. We believe that you can deploy and configure them without additional instructions, or you are able to find them on your own.","title":"Who are you?"},{"location":"azure-policy-continous-integration/#almost-step-by-step-guide","text":"","title":"Almost step by step guide."},{"location":"azure-policy-continous-integration/#repository-hierarchy","text":"\u00a6 \u00a6 README.md \u00a6 +---arm \u00a6 ResourceGroups.json \u00a6 TaggingGovernance.json \u00a6 +---scripts Tagging.Tests.ps1 TaggingGovernance.json is a placeholder for Azure Policy implementation. { \"$schema\" : \"https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#\" , \"contentVersion\" : \"1.0.0.0\" , \"parameters\" : {}, \"resources\" : [] }","title":"Repository hierarchy."},{"location":"azure-policy-continous-integration/#pipeline","text":"","title":"Pipeline"},{"location":"azure-policy-continous-integration/#pipeline-details","text":"","title":"Pipeline details"},{"location":"azure-policy-continous-integration/#cleanup-current-azure-policy-assignment-script","text":"Get-AzPolicyAssignment | ` Where-Object -Property Name -NE 'SecurityCenterBuiltIn' | ` Remove-AzPolicyAssignment","title":"cleanup current azure policy assignment - script"},{"location":"azure-policy-continous-integration/#install-pester-script","text":"Install-Module -Name Pester -Force -SkipPublisherCheck","title":"install pester - script"},{"location":"azure-policy-continous-integration/#run-tests-script","text":"Set-Location -Path $Env:BUILD_SOURCESDIRECTORY Invoke-Pester -OutputFile \"./testresult.xml\" -OutputFormat 'NUnitXML'","title":"run tests - script"},{"location":"azure-policy-continous-integration/#publish-test-result","text":"","title":"publish test result"},{"location":"azure-policy-continous-integration/#it-should-like-similar","text":"","title":"It should like similar:"},{"location":"azure-policy-continous-integration/#branch-policy","text":"","title":"Branch policy"},{"location":"copy-database-between-stages/","text":"Copy Azure database between stages Goal of this article is to show you how to easily migrate database between two Azure SQL Servers. It can be usefull in scenarios where you need to have exact configuration on two environments and want to have it done in automatic way. However you should bear in mind that for big size databases process of migration can take quite long. What services are needed to have solution working properly? The only service which is mandatory (besides two Azure SQL Servers) is Azure Storage Account, on which bacpac files will be stored. It's up to you how you will configure this storage, as there aren't any specific requirement. How the process of migration looks like? As you probably saw on main picture of this article the process is quite easy: Script initialize connection to source database. Script is starting export of bacpac file from source database to storage account. Script checks if bacpac was properly created. Script is starting import of bacpac file to destination database from storage account. Script validate if process of importing bacpac file finished with success. Prerequisites Az module installed on platform on which script will be run (eg. your local PC, Azure DevOps pipeline, Azure Automation) Install-Module -Name Az -Force Firewall rule added on SQL Server (if you are using local PC) or option \"Allow Azure services and resources to access this server\" in case you use some other Azure service to run the script. Blob container called \"bacpacs\" created on storage account. Account/Service Prinicipal under which script will work should have proper permission (Contributor) assigned to storage account. Script: Copy database between stages Source Code param ( $sourceSqlUser , $sourceSubscriptionID , $sourceSqlServerResourceGroup , $sourceSqlServerName , $sourceSqlDatabaseName , $destinationDatabaseEdition , $destinationServiceObjectiveName , $destinationDatabaseMaxSizeBytes , $destinationSqlUser , $destinationSubscriptionID , $destinationSqlServerResourceGroup , $destinationSqlServerName , $destinationSqlDatabaseName , $commonSubscriptionId , $storageAccountName , $storageAccountResourceGroup , $keyVaultName , $sqlServerSourceKeyVaultEntry , $sqlServerDestinationKeyVaultEntry , $tenantId ) Connect-AzAccount -TenantId $tenantId Select-AzSubscription -SubscriptionId $commonSubscriptionID | Out-Null $blobUrl = ( Get-AzStorageAccount -ResourceGroupName $storageAccountResourceGroup -Name $storageAccountName | Select-Object *). Context . BlobEndpoint $storageAccountKey = ( Get-AzStorageAccountKey -ResourceGroupName $storageAccountResourceGroup -Name $storageAccountName )[ 0 ]. Value $sourceSqlServerPassword = ( Get-AzKeyVaultSecret -VaultName $keyVaultName -Name $sqlServerSourceKeyVaultEntry ). SecretValue $destinationSqlServerPassword = ( Get-AzKeyVaultSecret -VaultName $KeyVaultName -Name $sqlServerDestinationKeyVaultEntry ). SecretValue Select-AzSubscription -SubscriptionId $sourceSubscriptionID | Out-Null Try { Get-AzSqlDatabase -DatabaseName $sourceSqlDatabaseName -ServerName $sourceSqlServerName -ResourceGroupName $sourceSqlServerResourceGroup Write-Output \"Found $sourceSqlDatabaseName database on SQL Server $sourceSqlServerName\" } Catch { Write-Output \"Database $sourceSqlDatabaseName can not be found on SQL Server $sourceSqlServerName\" exit } $date = Get-Date -Format yyyyMMdd $url = $blobUrl + \"bacpacs/$sourceSqlDatabaseName_$date.bacpac\" $exportRequest = New-AzSqlDatabaseExport -DatabaseName $sourceSqlDatabaseName -ResourceGroupName $sourceSqlServerResourceGroup -ServerName $sourceSqlServerName -StorageKeyType \"StorageAccessKey\" -StorageUri $url -StorageKey $shdStorageAccountKey -AdministratorLogin $sourceSqlUser -AdministratorLoginPassword $sourceSqlServerPassword $exportStatus = Get-AzSqlDatabaseImportExportStatus -OperationStatusLink $exportRequest . OperationStatusLink Write-Output \"Exporting database $sourceSqlDatabaseName to bacpac file\" while ( $exportStatus . Status -eq \"InProgress\" ) { Start-Sleep -s 10 $exportStatus = Get-AzSqlDatabaseImportExportStatus -OperationStatusLink $exportRequest . OperationStatusLink Write-Output \"Exporting...\" } Write-Output \"Proceeding with destination database...\" Select-AzSubscription -SubscriptionId $destinationSubscriptionID | Out-Null if ( $exportStatus . Status -eq \"Succeeded\" ) { Write-Output \"Database $sourceSqlDatabaseName export to bacpac file finished with success.\" $targetDatabase = Get-AzSqlDatabase -DatabaseName $destinationSqlDatabaseName -ResourceGroupName $destinationSqlServerResourceGroup -ServerName $destinationSqlServerName if ( $targetDatabase ) { Try { Write-Output \"Removing database $destinationSqlDatabaseName on target server as it already exist.\" Remove-AzSqlDatabase -DatabaseName $destinationSqlDatabaseName -ResourceGroupName $destinationSqlServerResourceGroup -ServerName $destinationSqlServerName Write-Output \"Database $destinationSqlDatabaseName removal finished with success\" } Catch { Write-Output \"Unexpected error occured during removal of database $destinationSqlDatabaseName from server $destinationSqlServerName . Error: $( $_ . Exception . Message ) \" } } $importRequest = New-AzSqlDatabaseImport -DatabaseName $destinationSqlDatabaseName -ResourceGroupName $destinationSqlServerResourceGroup -ServerName $destinationSqlServerName -StorageKeyType \"StorageAccessKey\" -StorageUri $url -StorageKey $storageAccountKey -AdministratorLogin $destinationSqlUser -AdministratorLoginPassword $destinationSqlServerPassword -Edition $destinationDatabaseEdition -ServiceObjectiveName $destinationServiceObjectiveName -DatabaseMaxSizeBytes $destinationDatabaseMaxSizeBytes $importStatus = Get-AzSqlDatabaseImportExportStatus -OperationStatusLink $importRequest . OperationStatusLink Write-Output \"Importing database $destinationSqlDatabaseName from bacpac file\" while ( $importStatus . Status -eq \"InProgress\" ) { Start-Sleep -s 10 $importStatus = Get-AzSqlDatabaseImportExportStatus -OperationStatusLink $importRequest . OperationStatusLink Write-Output \"Importing...\" } if ( $importStatus . Status -eq \"Succeeded\" ) { Write-Output \"Database $destinationSqlDatabaseName import finished with success.\" } else { Write-Output \"Import of database $destinationSqlDatabaseName to SQL server $destinationSqlServerName failed. Error: $( $importStatus . StatusMessage ) \" } } else { Write-Output \"Export of database $destinationSqlDatabaseName from SQL server $destinationSqlServerName failed. Error: $( $exportStatus . StatusMessage ) \" } Things to have in mind before applying this solution Script will automatically remove database in the destination if it already exist, ensure that it can be done before running script. Check if database can be copied to different SQL server as it can contain some data which are sensitive and should be used only in specific environment. Ensure that provided secrets names are created and you have proper access policy configured for you account/service principal.","title":"Copy Azure database between stages"},{"location":"copy-database-between-stages/#copy-azure-database-between-stages","text":"Goal of this article is to show you how to easily migrate database between two Azure SQL Servers. It can be usefull in scenarios where you need to have exact configuration on two environments and want to have it done in automatic way. However you should bear in mind that for big size databases process of migration can take quite long.","title":"Copy Azure database between stages"},{"location":"copy-database-between-stages/#what-services-are-needed-to-have-solution-working-properly","text":"The only service which is mandatory (besides two Azure SQL Servers) is Azure Storage Account, on which bacpac files will be stored. It's up to you how you will configure this storage, as there aren't any specific requirement.","title":"What services are needed to have solution working properly?"},{"location":"copy-database-between-stages/#how-the-process-of-migration-looks-like","text":"As you probably saw on main picture of this article the process is quite easy: Script initialize connection to source database. Script is starting export of bacpac file from source database to storage account. Script checks if bacpac was properly created. Script is starting import of bacpac file to destination database from storage account. Script validate if process of importing bacpac file finished with success.","title":"How the process of migration looks like?"},{"location":"copy-database-between-stages/#prerequisites","text":"Az module installed on platform on which script will be run (eg. your local PC, Azure DevOps pipeline, Azure Automation) Install-Module -Name Az -Force Firewall rule added on SQL Server (if you are using local PC) or option \"Allow Azure services and resources to access this server\" in case you use some other Azure service to run the script. Blob container called \"bacpacs\" created on storage account. Account/Service Prinicipal under which script will work should have proper permission (Contributor) assigned to storage account.","title":"Prerequisites"},{"location":"copy-database-between-stages/#script","text":"Copy database between stages Source Code param ( $sourceSqlUser , $sourceSubscriptionID , $sourceSqlServerResourceGroup , $sourceSqlServerName , $sourceSqlDatabaseName , $destinationDatabaseEdition , $destinationServiceObjectiveName , $destinationDatabaseMaxSizeBytes , $destinationSqlUser , $destinationSubscriptionID , $destinationSqlServerResourceGroup , $destinationSqlServerName , $destinationSqlDatabaseName , $commonSubscriptionId , $storageAccountName , $storageAccountResourceGroup , $keyVaultName , $sqlServerSourceKeyVaultEntry , $sqlServerDestinationKeyVaultEntry , $tenantId ) Connect-AzAccount -TenantId $tenantId Select-AzSubscription -SubscriptionId $commonSubscriptionID | Out-Null $blobUrl = ( Get-AzStorageAccount -ResourceGroupName $storageAccountResourceGroup -Name $storageAccountName | Select-Object *). Context . BlobEndpoint $storageAccountKey = ( Get-AzStorageAccountKey -ResourceGroupName $storageAccountResourceGroup -Name $storageAccountName )[ 0 ]. Value $sourceSqlServerPassword = ( Get-AzKeyVaultSecret -VaultName $keyVaultName -Name $sqlServerSourceKeyVaultEntry ). SecretValue $destinationSqlServerPassword = ( Get-AzKeyVaultSecret -VaultName $KeyVaultName -Name $sqlServerDestinationKeyVaultEntry ). SecretValue Select-AzSubscription -SubscriptionId $sourceSubscriptionID | Out-Null Try { Get-AzSqlDatabase -DatabaseName $sourceSqlDatabaseName -ServerName $sourceSqlServerName -ResourceGroupName $sourceSqlServerResourceGroup Write-Output \"Found $sourceSqlDatabaseName database on SQL Server $sourceSqlServerName\" } Catch { Write-Output \"Database $sourceSqlDatabaseName can not be found on SQL Server $sourceSqlServerName\" exit } $date = Get-Date -Format yyyyMMdd $url = $blobUrl + \"bacpacs/$sourceSqlDatabaseName_$date.bacpac\" $exportRequest = New-AzSqlDatabaseExport -DatabaseName $sourceSqlDatabaseName -ResourceGroupName $sourceSqlServerResourceGroup -ServerName $sourceSqlServerName -StorageKeyType \"StorageAccessKey\" -StorageUri $url -StorageKey $shdStorageAccountKey -AdministratorLogin $sourceSqlUser -AdministratorLoginPassword $sourceSqlServerPassword $exportStatus = Get-AzSqlDatabaseImportExportStatus -OperationStatusLink $exportRequest . OperationStatusLink Write-Output \"Exporting database $sourceSqlDatabaseName to bacpac file\" while ( $exportStatus . Status -eq \"InProgress\" ) { Start-Sleep -s 10 $exportStatus = Get-AzSqlDatabaseImportExportStatus -OperationStatusLink $exportRequest . OperationStatusLink Write-Output \"Exporting...\" } Write-Output \"Proceeding with destination database...\" Select-AzSubscription -SubscriptionId $destinationSubscriptionID | Out-Null if ( $exportStatus . Status -eq \"Succeeded\" ) { Write-Output \"Database $sourceSqlDatabaseName export to bacpac file finished with success.\" $targetDatabase = Get-AzSqlDatabase -DatabaseName $destinationSqlDatabaseName -ResourceGroupName $destinationSqlServerResourceGroup -ServerName $destinationSqlServerName if ( $targetDatabase ) { Try { Write-Output \"Removing database $destinationSqlDatabaseName on target server as it already exist.\" Remove-AzSqlDatabase -DatabaseName $destinationSqlDatabaseName -ResourceGroupName $destinationSqlServerResourceGroup -ServerName $destinationSqlServerName Write-Output \"Database $destinationSqlDatabaseName removal finished with success\" } Catch { Write-Output \"Unexpected error occured during removal of database $destinationSqlDatabaseName from server $destinationSqlServerName . Error: $( $_ . Exception . Message ) \" } } $importRequest = New-AzSqlDatabaseImport -DatabaseName $destinationSqlDatabaseName -ResourceGroupName $destinationSqlServerResourceGroup -ServerName $destinationSqlServerName -StorageKeyType \"StorageAccessKey\" -StorageUri $url -StorageKey $storageAccountKey -AdministratorLogin $destinationSqlUser -AdministratorLoginPassword $destinationSqlServerPassword -Edition $destinationDatabaseEdition -ServiceObjectiveName $destinationServiceObjectiveName -DatabaseMaxSizeBytes $destinationDatabaseMaxSizeBytes $importStatus = Get-AzSqlDatabaseImportExportStatus -OperationStatusLink $importRequest . OperationStatusLink Write-Output \"Importing database $destinationSqlDatabaseName from bacpac file\" while ( $importStatus . Status -eq \"InProgress\" ) { Start-Sleep -s 10 $importStatus = Get-AzSqlDatabaseImportExportStatus -OperationStatusLink $importRequest . OperationStatusLink Write-Output \"Importing...\" } if ( $importStatus . Status -eq \"Succeeded\" ) { Write-Output \"Database $destinationSqlDatabaseName import finished with success.\" } else { Write-Output \"Import of database $destinationSqlDatabaseName to SQL server $destinationSqlServerName failed. Error: $( $importStatus . StatusMessage ) \" } } else { Write-Output \"Export of database $destinationSqlDatabaseName from SQL server $destinationSqlServerName failed. Error: $( $exportStatus . StatusMessage ) \" }","title":"Script:"},{"location":"copy-database-between-stages/#things-to-have-in-mind-before-applying-this-solution","text":"Script will automatically remove database in the destination if it already exist, ensure that it can be done before running script. Check if database can be copied to different SQL server as it can contain some data which are sensitive and should be used only in specific environment. Ensure that provided secrets names are created and you have proper access policy configured for you account/service principal.","title":"Things to have in mind before applying this solution"},{"location":"multi-stage-yaml-pipeline-configuration/","text":"Multi-Stage YAML Pipeline Configuration Since May 2019 Microsoft offer as a part of Azure DevOps service Multi-Stage YAML pipelines. Probably you will ask what is a difference between standard approach of CICD process and Multi-Stage? Answer is pretty simple - in standard approach only CI (build) part of CICD process was possible to define as YAML definition. Multi-Stage YAML pipelines (name of it is bacially self explanatory) approach allows to define both CI (build) and CD(release) processes in YAML language. Ok, but why I should use it if I can easly click everything in portal? Of course you can do everything via GUI, but common why not take one step forward and define your pipeline as a code? How cool it can be if you are defining not only infrastructure as a code but also your whole CICD process? If I didn't encourage you yet to using this funcionality, below you can find more advantages of using it: Diff option - you can compare the definition which are failing with the last known good configuration. History - source control allows you to see every change which was done to your pipeline since the initial creation. Rollback - if you found that your last commit causing any problem during deployment, simple roll it back to last good configuration. Reusability option - how often you wanted to reuse pipeline which is already defined? - now you can simply do that by copy/paste option. Team cooperation - if there are multiple people working on same pipeline it can cause problem using GUI, using YAML team members can work on separate branch and adjust definition according to their needs. Sounds good, but what if I don't have experience with YAML language? If you think it is problem you are totally wrong. YAML is not complicated - if you ever define Azure resource definition using Azure Resource Manager (ARM template) you will for sure be able to easily write YAML pipelines. How to get started with Multi-Stage YAML pipelines? Let me guide you step by step how to configure such pipeline. In my example I will show you how to configure Azure Kubernetes Service with some additional tools which are usefull during application deployment. Later you can adjust it to your needs by changing specific task - however overall process of creation will be basically the same. Prerequisites In my scenario I will shouw you how to setup additional tools on Azure Kubernetes Service cluster . Following prerequisites should be met: At least one AKS cluster should be created Proper permission assigned for AKS cluster - best to have Owner In first step we need to define environment in Azure Dev Ops for each cluster. Go to Pipeline->Environment->Create Environment Type the name of the environment, provide description and choose Kubernetes as a resource and click Next. In next window choose subscription and AKS cluster which you want to add for specific environment. Define also namespace which should be used and click validate and create. Once it's done you should see AKS cluster added to environment. Repeat above steps if you want to add additional AKS clusters. You should also configure variable group which should look like on below picture. Of course values for environments and service connections should be adjusted to your configuration. YAML configuration Once we have environment configured we can start with configuration of YAML files. Build configuration As a first step we should define build pipeline azure-pipelines.yaml . In our scenario it will be quite easy and yaml will look like this: variables : - group : AKSBuildVariables stages : - stage : Build jobs : - job : Build pool : vmImage : 'windows-2019' continueOnError : true steps : - checkout : self clean : true persistCredentials : true - task : PublishPipelineArtifact@1 inputs : path : $(System.DefaultWorkingDirectory) artifact : yaml - template : deploy-to-all-stages.yaml As a variables we are using variable group defined in prerequisites step. It will be used for gathering all sensitive data in encrypted way. The main thing we will do here is publish all files as artifacts of this build. At the end of above yaml there is template deploy-to-all-stages.yaml which will be used for caling specific stages. deploy-to-all-stages.yaml configuration stages : - template : deployment-stage.yaml parameters : STAGE_NAME : DEV STAGE_ENVIRONMENT : $(DEV_ENVIRONMENT_NAME) STAGE_K8S_SERVICE_ENDPOINT : $(DEV_CLUSTER_SERVICE_CONNECTION_NAME) - template : deployment-stage.yaml parameters : STAGE_NAME : PROD STAGE_ENVIRONMENT : $(PROD_ENVIRONMENT_NAME) STAGE_K8S_SERVICE_ENDPOINT : $(PROD_CLUSTER_SERVICE_CONNECTION_NAME) As you can see here we are specifing configuration for each stage. We can use different deployment template depends on scenario we want to implement. To make it easier I will use same template file for both stages, only input parameters will be different. Parameters will be taken directly from variable group defined in build part. If you want to use different variable group for each environment you can define them as well on deployment template level. Once you have file with stages defined, it's time to create main deployment template. In our case it will be called deployment-stage.yaml . deployment-stage.yaml configuration parameters : STAGE_NAME : '' STAGE_ENVIRONMENT : '' STAGE_K8S_SERVICE_ENDPOINT : '' stages : - stage : ${{ parameters.STAGE_NAME }} jobs : - deployment : SetupCluster pool : vmImage : 'ubuntu-latest' environment : ${{ parameters.STAGE_ENVIRONMENT }} strategy : runOnce : deploy : steps : - task : DownloadPipelineArtifact@2 inputs : artifactName : yaml targetPath : $(Build.SourcesDirectory)/yaml - task : HelmInstaller@1 inputs : helmVersionToInstall : '2.14.3' - task : Kubernetes@1 displayName : Configure custom role inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/configure-custom-role.yaml.yaml' - task : Bash@3 displayName : \"Repo add nginx\" inputs : targetType : 'inline' script : 'helm repo add nginx-stable https://helm.nginx.com/stable' - task : Bash@3 displayName : \"Repo update\" inputs : targetType : 'inline' script : 'helm repo update' Deployment stage template starts with input parameters. Values of those parameters are taken from deploy-to-all-stages.yaml separately for each stage. To use input parameters please fallow syntax like below: ${{parameters.INPUT_PARAMETER }} Next we are defining vm image on which deployment will take place and main configuration is done. In deploy section we provide tasks which should be used in our release. In our case we will perform following operations: - Download artifacts from build - Install Helm tool - Create a custom role and assign it to group system:serviceaccounts - Download nginx (without installation) to repository on AKS node - Update repository on AKS node For third and last task we need additional yaml file to be added to our GIT reposiotry. configure-custom-role.yaml kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : namespace : default name : azure-devops-deploy-role rules : - apiGroups : [ \"\" , \"extensions\" , \"apps\" ] resources : [ \"deployments\" , \"replicasets\" , \"pods\" , \"services\" ] verbs : [ \"get\" , \"list\" , \"watch\" , \"create\" , \"update\" , \"patch\" , \"delete\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : azure-devops-deploy-manager namespace : default subjects : - kind : Group name : system:serviceaccounts apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : azure-devops-deploy-role apiGroup : rbac.authorization.k8s.io Linking a build yaml file in Azure DevOps Ok, once we have everything prepared your GIT repo should looks like on below picture. Let's go now to Azure DevOps and configure pipeline. Go to Pipeline->Create Pipeline-> Azure Repos Git -> Select name of your repo -> Exisitng Azure Pipelines Yaml File -> Select azure-pipelines.yaml from your GIT repo and click on Run button. As a result you should see that Multi-Stage YAML pipeline apply changes to all your clusters. Conclusion In my example I shared with you scenario for AKS cluster configuration. Similar scenarios for deployment with ARM template usage can be done similar way without going deep dive into envrionments part. Just wanted to show you how awesome are multi-stage YAML pipelines, hope that I ecourage you to use them ;)","title":"Multi-Stage YAML Pipeline Configuration"},{"location":"multi-stage-yaml-pipeline-configuration/#multi-stage-yaml-pipeline-configuration","text":"Since May 2019 Microsoft offer as a part of Azure DevOps service Multi-Stage YAML pipelines. Probably you will ask what is a difference between standard approach of CICD process and Multi-Stage? Answer is pretty simple - in standard approach only CI (build) part of CICD process was possible to define as YAML definition. Multi-Stage YAML pipelines (name of it is bacially self explanatory) approach allows to define both CI (build) and CD(release) processes in YAML language.","title":"Multi-Stage YAML Pipeline Configuration"},{"location":"multi-stage-yaml-pipeline-configuration/#ok-but-why-i-should-use-it-if-i-can-easly-click-everything-in-portal","text":"Of course you can do everything via GUI, but common why not take one step forward and define your pipeline as a code? How cool it can be if you are defining not only infrastructure as a code but also your whole CICD process? If I didn't encourage you yet to using this funcionality, below you can find more advantages of using it: Diff option - you can compare the definition which are failing with the last known good configuration. History - source control allows you to see every change which was done to your pipeline since the initial creation. Rollback - if you found that your last commit causing any problem during deployment, simple roll it back to last good configuration. Reusability option - how often you wanted to reuse pipeline which is already defined? - now you can simply do that by copy/paste option. Team cooperation - if there are multiple people working on same pipeline it can cause problem using GUI, using YAML team members can work on separate branch and adjust definition according to their needs.","title":"Ok, but why I should use it if I can easly click everything in portal?"},{"location":"multi-stage-yaml-pipeline-configuration/#sounds-good-but-what-if-i-dont-have-experience-with-yaml-language","text":"If you think it is problem you are totally wrong. YAML is not complicated - if you ever define Azure resource definition using Azure Resource Manager (ARM template) you will for sure be able to easily write YAML pipelines.","title":"Sounds good, but what if I don't have experience with YAML language?"},{"location":"multi-stage-yaml-pipeline-configuration/#how-to-get-started-with-multi-stage-yaml-pipelines","text":"Let me guide you step by step how to configure such pipeline. In my example I will show you how to configure Azure Kubernetes Service with some additional tools which are usefull during application deployment. Later you can adjust it to your needs by changing specific task - however overall process of creation will be basically the same.","title":"How to get started with Multi-Stage YAML pipelines?"},{"location":"multi-stage-yaml-pipeline-configuration/#prerequisites","text":"In my scenario I will shouw you how to setup additional tools on Azure Kubernetes Service cluster . Following prerequisites should be met: At least one AKS cluster should be created Proper permission assigned for AKS cluster - best to have Owner In first step we need to define environment in Azure Dev Ops for each cluster. Go to Pipeline->Environment->Create Environment Type the name of the environment, provide description and choose Kubernetes as a resource and click Next. In next window choose subscription and AKS cluster which you want to add for specific environment. Define also namespace which should be used and click validate and create. Once it's done you should see AKS cluster added to environment. Repeat above steps if you want to add additional AKS clusters. You should also configure variable group which should look like on below picture. Of course values for environments and service connections should be adjusted to your configuration.","title":"Prerequisites"},{"location":"multi-stage-yaml-pipeline-configuration/#yaml-configuration","text":"Once we have environment configured we can start with configuration of YAML files.","title":"YAML configuration"},{"location":"multi-stage-yaml-pipeline-configuration/#build-configuration","text":"As a first step we should define build pipeline azure-pipelines.yaml . In our scenario it will be quite easy and yaml will look like this: variables : - group : AKSBuildVariables stages : - stage : Build jobs : - job : Build pool : vmImage : 'windows-2019' continueOnError : true steps : - checkout : self clean : true persistCredentials : true - task : PublishPipelineArtifact@1 inputs : path : $(System.DefaultWorkingDirectory) artifact : yaml - template : deploy-to-all-stages.yaml As a variables we are using variable group defined in prerequisites step. It will be used for gathering all sensitive data in encrypted way. The main thing we will do here is publish all files as artifacts of this build. At the end of above yaml there is template deploy-to-all-stages.yaml which will be used for caling specific stages.","title":"Build configuration"},{"location":"multi-stage-yaml-pipeline-configuration/#deploy-to-all-stagesyaml-configuration","text":"stages : - template : deployment-stage.yaml parameters : STAGE_NAME : DEV STAGE_ENVIRONMENT : $(DEV_ENVIRONMENT_NAME) STAGE_K8S_SERVICE_ENDPOINT : $(DEV_CLUSTER_SERVICE_CONNECTION_NAME) - template : deployment-stage.yaml parameters : STAGE_NAME : PROD STAGE_ENVIRONMENT : $(PROD_ENVIRONMENT_NAME) STAGE_K8S_SERVICE_ENDPOINT : $(PROD_CLUSTER_SERVICE_CONNECTION_NAME) As you can see here we are specifing configuration for each stage. We can use different deployment template depends on scenario we want to implement. To make it easier I will use same template file for both stages, only input parameters will be different. Parameters will be taken directly from variable group defined in build part. If you want to use different variable group for each environment you can define them as well on deployment template level. Once you have file with stages defined, it's time to create main deployment template. In our case it will be called deployment-stage.yaml .","title":"deploy-to-all-stages.yaml configuration"},{"location":"multi-stage-yaml-pipeline-configuration/#deployment-stageyaml-configuration","text":"parameters : STAGE_NAME : '' STAGE_ENVIRONMENT : '' STAGE_K8S_SERVICE_ENDPOINT : '' stages : - stage : ${{ parameters.STAGE_NAME }} jobs : - deployment : SetupCluster pool : vmImage : 'ubuntu-latest' environment : ${{ parameters.STAGE_ENVIRONMENT }} strategy : runOnce : deploy : steps : - task : DownloadPipelineArtifact@2 inputs : artifactName : yaml targetPath : $(Build.SourcesDirectory)/yaml - task : HelmInstaller@1 inputs : helmVersionToInstall : '2.14.3' - task : Kubernetes@1 displayName : Configure custom role inputs : connectionType : 'Kubernetes Service Connection' kubernetesServiceEndpoint : ${{ parameters.STAGE_K8S_SERVICE_ENDPOINT }} command : 'apply' arguments : '-f $(Build.SourcesDirectory)/yaml/configure-custom-role.yaml.yaml' - task : Bash@3 displayName : \"Repo add nginx\" inputs : targetType : 'inline' script : 'helm repo add nginx-stable https://helm.nginx.com/stable' - task : Bash@3 displayName : \"Repo update\" inputs : targetType : 'inline' script : 'helm repo update' Deployment stage template starts with input parameters. Values of those parameters are taken from deploy-to-all-stages.yaml separately for each stage. To use input parameters please fallow syntax like below: ${{parameters.INPUT_PARAMETER }} Next we are defining vm image on which deployment will take place and main configuration is done. In deploy section we provide tasks which should be used in our release. In our case we will perform following operations: - Download artifacts from build - Install Helm tool - Create a custom role and assign it to group system:serviceaccounts - Download nginx (without installation) to repository on AKS node - Update repository on AKS node For third and last task we need additional yaml file to be added to our GIT reposiotry. configure-custom-role.yaml kind : Role apiVersion : rbac.authorization.k8s.io/v1 metadata : namespace : default name : azure-devops-deploy-role rules : - apiGroups : [ \"\" , \"extensions\" , \"apps\" ] resources : [ \"deployments\" , \"replicasets\" , \"pods\" , \"services\" ] verbs : [ \"get\" , \"list\" , \"watch\" , \"create\" , \"update\" , \"patch\" , \"delete\" ] --- kind : RoleBinding apiVersion : rbac.authorization.k8s.io/v1 metadata : name : azure-devops-deploy-manager namespace : default subjects : - kind : Group name : system:serviceaccounts apiGroup : rbac.authorization.k8s.io roleRef : kind : Role name : azure-devops-deploy-role apiGroup : rbac.authorization.k8s.io","title":"deployment-stage.yaml configuration"},{"location":"multi-stage-yaml-pipeline-configuration/#linking-a-build-yaml-file-in-azure-devops","text":"Ok, once we have everything prepared your GIT repo should looks like on below picture. Let's go now to Azure DevOps and configure pipeline. Go to Pipeline->Create Pipeline-> Azure Repos Git -> Select name of your repo -> Exisitng Azure Pipelines Yaml File -> Select azure-pipelines.yaml from your GIT repo and click on Run button. As a result you should see that Multi-Stage YAML pipeline apply changes to all your clusters.","title":"Linking a build yaml file in Azure DevOps"},{"location":"multi-stage-yaml-pipeline-configuration/#conclusion","text":"In my example I shared with you scenario for AKS cluster configuration. Similar scenarios for deployment with ARM template usage can be done similar way without going deep dive into envrionments part. Just wanted to show you how awesome are multi-stage YAML pipelines, hope that I ecourage you to use them ;)","title":"Conclusion"},{"location":"powershell-web-browser-automation/","text":"Web browser automation with Powershell & Selenium After reading this article, you should be able to write a basic web-automation script on your own. We hope you already have an idea where you want to use it. $cred = Get-Credential .\\ New-AzPayAsYouGoSubscription . ps1 -Credentials $cred Important Although this article shows how to automate creating a new PAYG subscription, it's goal is to present web automation with PowerShell and Selenium. Please consider the example script as a byproduct. First of all: it's easy! You need Powershell Selenium module from Powershell Gallery . Install-Module -Name Selenium -RequiredVersion 2 . 3 . 1 Every single step of the entire automation process is more or less: Navigation to URL. Waiting for one or more element/s like input, button, etc. to appear. Doing action: filling a form, clicking a button, etc. * Steps 2 and 3 require you to know the way to identify elements. How to navigate to URL? $Url = 'http://example.com' $Driver = Start-SeChrome Enter-SeUrl -Driver $Driver -Url $Url How to identify elements? Use developer tools included in your web browser. I use chrome. After clicking right on an element of interest, you need to select inspect. Then you'll get details and can decide whether to identify an item by id, class, etc. $UsernameElementName = 'loginfmt' $NameElement = Find-SeElement -Driver $Driver -Name $UsernameElementName How to wait for the element to appear? Any user knows that you need to see the button before you can click it. The same goes here. $PaygPlanElementClassName = 'plan_type_consumption' $PaygPlanElement = Find-SeElement -Driver $Driver -ClassName while ( -not $PaygPlanElement ) { Write-Host -Object \"Waiting for element class $( $PaygPlanElementClassName ) \" Start-Sleep -Seconds 1 $PaygPlanElement = Find-SeElement -Driver $Driver -ClassName $PaygPlanElementClassName } How to perform item-sepcific action? $PasswordElement = Find-SeElement -Driver $Driver -Name 'passwd' while ( -not $PasswordElement ) { Start-Sleep -Seconds 1 $PasswordElement = Find-SeElement -Driver $Driver -Name 'passwd' } Send-SeKeys -Element $PasswordElement -Keys $Credentials . GetNetworkCredential (). Password ( Find-SeElement -Driver $Driver -Id 'idSIButton9' ). Click () Do you want to perform more than one 'wait and click' operations in a row? Use the following method if there is a need to perform the same action on elements that can be selected by the same property. $ElementsToClick = @( 'card-submit-button' , 'no-support-option' , 'attach-support-button' ) foreach ( $elementId in $ElementsToClick ) { do { $Element = Find-SeElement -Driver $Driver -Id $elementId if ( $Element ){ try { $Element . Click () } catch { $Element = $null } } else { Write-Host -Object \"Waiting for element id $( $elementId ) \" Start-Sleep -Seconds 1 } } while ( -not $Element ) } Script https://github.com/automationgurus/automationgurus.github.io/blob/master/src/code/New-AzPayAsYouGoSubscription.ps1 param ( $Credentials ) Write-Host -Object \"Start\" $Url = \"https://account.azure.com/signup?showCatalog=True&appId=Ibiza_SubscriptionsOverviewBladeCommandBar\" $Driver = Start-SeChrome Enter-SeUrl -Driver $Driver -Url $Url #Enter username $UsernameElementName = 'loginfmt' $UsernameElement = Find-SeElement -Driver $Driver -Name $UsernameElementName while ( -not $UsernameElement ) { Write-Host -Object \"Waiting for element $( $UsernameElementName ) \" Start-Sleep -Seconds 1 $UsernameElement = Find-SeElement -Driver $Driver -Name $UsernameElementName } Send-SeKeys -Element $UsernameElement -Keys $Credentials . UserName ( Find-SeElement -Driver $Driver -Id 'idSIButton9' ). Click () #Select 'Personal Account' $PersonalAccountElement = Find-SeElement -Driver $Driver -Id 'msaTile' while ( -not $PersonalAccountElement ) { Start-Sleep -Seconds 1 $PersonalAccountElement = Find-SeElement -Driver $Driver -Id 'msaTile' } $PersonalAccountElement . Click () #Enter password $PasswordElementName = 'passwd' $PasswordElement = Find-SeElement -Driver $Driver -Name $PasswordElementName while ( -not $PasswordElement ) { Write-Host -Object \"Waiting for element name $( $PasswordElementName ) \" Start-Sleep -Seconds 1 $PasswordElement = Find-SeElement -Driver $Driver -Name $PasswordElementName } Send-SeKeys -Element $PasswordElement -Keys $Credentials . GetNetworkCredential (). Password ( Find-SeElement -Driver $Driver -Id 'idSIButton9' ). Click () #Select PAYG Plan $PaygPlanElementClassName = 'plan_type_consumption' $PaygPlanElement = Find-SeElement -Driver $Driver -ClassName $PaygPlanElementClassName | Where-Object -Property Text -like '*Pay-As-You-Go*' while ( -not $PaygPlanElement ) { Write-Host -Object \"Waiting for element class $( $PaygPlanElementClassName ) \" Start-Sleep -Seconds 1 $PaygPlanElement = Find-SeElement -Driver $Driver -ClassName $PaygPlanElementClassName | Where-Object -Property Text -like '*Pay-As-You-Go*' } $PaygPlanElement . Click () #Set Payment and support agreement $ElementsToClick = @( 'card-submit-button' , 'no-support-option' , 'attach-support-button' ) foreach ( $elementId in $ElementsToClick ) { do { $Element = Find-SeElement -Driver $Driver -Id $elementId if ( $Element ){ try { $Element . Click () } catch { $Element = $null } } else { Write-Host -Object \"Waiting for element id $( $elementId ) \" Start-Sleep -Seconds 1 } } while ( -not $Element ) } #Accept terms $AgreeElementId = 'accept-terms-checkbox' $AgreeElement = Find-SeElement -Driver $Driver -Id $AgreeElementId while ( -not $AgreeElement ) { Write-Host -Object \"Waiting for element id $( $AgreeElementId ) \" Start-Sleep -Seconds 1 $AgreeElement = Find-SeElement -Driver $Driver -Id $AgreeElementId } Send-SeKeys -Element $AgreeElement -Keys ' ' #Create subscription $AcceptElementId = 'accept-terms-submit-button' do { $AcceptElement = Find-SeElement -Driver $Driver -Id $AcceptElementId if ( $AcceptElement ){ try { $AcceptElement . Click () } catch { $AcceptElement = $null } } else { Write-Host -Object \"Waiting for element id $( $AcceptElementId ) \" Start-Sleep -Seconds 1 } } while ( -not $AcceptElement ) Write-Host -Object \"End\"","title":"Powershell Web-Based Automation"},{"location":"powershell-web-browser-automation/#web-browser-automation-with-powershell-selenium","text":"After reading this article, you should be able to write a basic web-automation script on your own. We hope you already have an idea where you want to use it. $cred = Get-Credential .\\ New-AzPayAsYouGoSubscription . ps1 -Credentials $cred","title":"Web browser automation with Powershell &amp; Selenium"},{"location":"powershell-web-browser-automation/#important","text":"Although this article shows how to automate creating a new PAYG subscription, it's goal is to present web automation with PowerShell and Selenium. Please consider the example script as a byproduct.","title":"Important"},{"location":"powershell-web-browser-automation/#first-of-all-its-easy","text":"You need Powershell Selenium module from Powershell Gallery . Install-Module -Name Selenium -RequiredVersion 2 . 3 . 1 Every single step of the entire automation process is more or less: Navigation to URL. Waiting for one or more element/s like input, button, etc. to appear. Doing action: filling a form, clicking a button, etc. * Steps 2 and 3 require you to know the way to identify elements.","title":"First of all: it's easy!"},{"location":"powershell-web-browser-automation/#how-to-navigate-to-url","text":"$Url = 'http://example.com' $Driver = Start-SeChrome Enter-SeUrl -Driver $Driver -Url $Url","title":"How to navigate to URL?"},{"location":"powershell-web-browser-automation/#how-to-identify-elements","text":"Use developer tools included in your web browser. I use chrome. After clicking right on an element of interest, you need to select inspect. Then you'll get details and can decide whether to identify an item by id, class, etc. $UsernameElementName = 'loginfmt' $NameElement = Find-SeElement -Driver $Driver -Name $UsernameElementName","title":"How to identify elements?"},{"location":"powershell-web-browser-automation/#how-to-wait-for-the-element-to-appear","text":"Any user knows that you need to see the button before you can click it. The same goes here. $PaygPlanElementClassName = 'plan_type_consumption' $PaygPlanElement = Find-SeElement -Driver $Driver -ClassName while ( -not $PaygPlanElement ) { Write-Host -Object \"Waiting for element class $( $PaygPlanElementClassName ) \" Start-Sleep -Seconds 1 $PaygPlanElement = Find-SeElement -Driver $Driver -ClassName $PaygPlanElementClassName }","title":"How to wait for the element to appear?"},{"location":"powershell-web-browser-automation/#how-to-perform-item-sepcific-action","text":"$PasswordElement = Find-SeElement -Driver $Driver -Name 'passwd' while ( -not $PasswordElement ) { Start-Sleep -Seconds 1 $PasswordElement = Find-SeElement -Driver $Driver -Name 'passwd' } Send-SeKeys -Element $PasswordElement -Keys $Credentials . GetNetworkCredential (). Password ( Find-SeElement -Driver $Driver -Id 'idSIButton9' ). Click ()","title":"How to perform item-sepcific action?"},{"location":"powershell-web-browser-automation/#do-you-want-to-perform-more-than-one-wait-and-click-operations-in-a-row","text":"Use the following method if there is a need to perform the same action on elements that can be selected by the same property. $ElementsToClick = @( 'card-submit-button' , 'no-support-option' , 'attach-support-button' ) foreach ( $elementId in $ElementsToClick ) { do { $Element = Find-SeElement -Driver $Driver -Id $elementId if ( $Element ){ try { $Element . Click () } catch { $Element = $null } } else { Write-Host -Object \"Waiting for element id $( $elementId ) \" Start-Sleep -Seconds 1 } } while ( -not $Element ) }","title":"Do you want to perform more than one 'wait and click' operations in a row?"},{"location":"powershell-web-browser-automation/#script","text":"https://github.com/automationgurus/automationgurus.github.io/blob/master/src/code/New-AzPayAsYouGoSubscription.ps1 param ( $Credentials ) Write-Host -Object \"Start\" $Url = \"https://account.azure.com/signup?showCatalog=True&appId=Ibiza_SubscriptionsOverviewBladeCommandBar\" $Driver = Start-SeChrome Enter-SeUrl -Driver $Driver -Url $Url #Enter username $UsernameElementName = 'loginfmt' $UsernameElement = Find-SeElement -Driver $Driver -Name $UsernameElementName while ( -not $UsernameElement ) { Write-Host -Object \"Waiting for element $( $UsernameElementName ) \" Start-Sleep -Seconds 1 $UsernameElement = Find-SeElement -Driver $Driver -Name $UsernameElementName } Send-SeKeys -Element $UsernameElement -Keys $Credentials . UserName ( Find-SeElement -Driver $Driver -Id 'idSIButton9' ). Click () #Select 'Personal Account' $PersonalAccountElement = Find-SeElement -Driver $Driver -Id 'msaTile' while ( -not $PersonalAccountElement ) { Start-Sleep -Seconds 1 $PersonalAccountElement = Find-SeElement -Driver $Driver -Id 'msaTile' } $PersonalAccountElement . Click () #Enter password $PasswordElementName = 'passwd' $PasswordElement = Find-SeElement -Driver $Driver -Name $PasswordElementName while ( -not $PasswordElement ) { Write-Host -Object \"Waiting for element name $( $PasswordElementName ) \" Start-Sleep -Seconds 1 $PasswordElement = Find-SeElement -Driver $Driver -Name $PasswordElementName } Send-SeKeys -Element $PasswordElement -Keys $Credentials . GetNetworkCredential (). Password ( Find-SeElement -Driver $Driver -Id 'idSIButton9' ). Click () #Select PAYG Plan $PaygPlanElementClassName = 'plan_type_consumption' $PaygPlanElement = Find-SeElement -Driver $Driver -ClassName $PaygPlanElementClassName | Where-Object -Property Text -like '*Pay-As-You-Go*' while ( -not $PaygPlanElement ) { Write-Host -Object \"Waiting for element class $( $PaygPlanElementClassName ) \" Start-Sleep -Seconds 1 $PaygPlanElement = Find-SeElement -Driver $Driver -ClassName $PaygPlanElementClassName | Where-Object -Property Text -like '*Pay-As-You-Go*' } $PaygPlanElement . Click () #Set Payment and support agreement $ElementsToClick = @( 'card-submit-button' , 'no-support-option' , 'attach-support-button' ) foreach ( $elementId in $ElementsToClick ) { do { $Element = Find-SeElement -Driver $Driver -Id $elementId if ( $Element ){ try { $Element . Click () } catch { $Element = $null } } else { Write-Host -Object \"Waiting for element id $( $elementId ) \" Start-Sleep -Seconds 1 } } while ( -not $Element ) } #Accept terms $AgreeElementId = 'accept-terms-checkbox' $AgreeElement = Find-SeElement -Driver $Driver -Id $AgreeElementId while ( -not $AgreeElement ) { Write-Host -Object \"Waiting for element id $( $AgreeElementId ) \" Start-Sleep -Seconds 1 $AgreeElement = Find-SeElement -Driver $Driver -Id $AgreeElementId } Send-SeKeys -Element $AgreeElement -Keys ' ' #Create subscription $AcceptElementId = 'accept-terms-submit-button' do { $AcceptElement = Find-SeElement -Driver $Driver -Id $AcceptElementId if ( $AcceptElement ){ try { $AcceptElement . Click () } catch { $AcceptElement = $null } } else { Write-Host -Object \"Waiting for element id $( $AcceptElementId ) \" Start-Sleep -Seconds 1 } } while ( -not $AcceptElement ) Write-Host -Object \"End\"","title":"Script"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/","text":"Security and Compliance as a Code: Azure Policy Test-Driven Development According to Wardley Map of Security Skills required by Industry Security and Compliance as a Code is now in Genesis. There can be no more exciting area to dive in. Generally speaking, we're inheriting more and more good practices known for years in the software development world. We've already adopted using source control repository to store our script and declarative definitions of our infrastructures. New tools are providing the capability to do static code analysis for non-classic programming languages. Infrastructure deployment is now a natural part of CD processes. Azure Marketplace is a place where you can find many tools supporting mentioned. What about the future? Let's start by using test-driven development in the process of providing consistency to our Azure subscriptions using Azure Policy . 'Who' is involved? Pester Powershell module Az.Resources Powershell module Azure Policy service Why develop automatic testing for Azure Policy? It's faster and more reliable. You'll find errors earlier. Therefore, fully implemented, well-working functionality will be delivered faster ! When any new Azure Policy is created, it will be easy to check if current requirements are still adequately implemented. You'll do the next modifications without affright. Finally, it can be proved that tests have been performed . Are you able to test hundreds of tests in a couple of minutes using Azure Portal? Test-Driven Development Process Implement tests (only this is covered in this article). Implement functionality. Show me that in action. As always, we want to explain it to you better using a practical use case. So we want to govern Resource Group tagging. Let's say that there are some requirements defined. More details Tag Name Required? Description Example Value Owner Yes Owner's email address kamil@automate.guru artur@automationgurus.github.io Environment Yes Can be only dev or prod dev, prod BusinessUnit Yes Division/Area/Team, all parts must be filled n itdivision/appsarea/csharpteam EndDate No Decommissioning date (DDMMYYYY) 29022024 ItsmRequestId No Service Request's ID sr0001, sr1990 A table and flow chart should be enough for you to understand what we want to achieve. Additional information: tag values must be lowercase, only allowed domains in the email address are 'automate.guru' and 'automationgurus.github.io'. Test procedure Create hashtable with some valid and invalid values, also put information if a particular tag presence is required. Assign initiative containing all policies enforcing business requirements at subscription scope. Wait 30 minutes to be sure that all assignments are applied. Use Test-Deployment Powershell to validate deployments in scenarios based on input from pt. 1. Checks results. Azure Resource Group deployment validation function We need a function that tries to create RG with tags provided by parameters. This function uses a universal ARM Template for deploying multiple RG. { \"$schema\" : \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"1.0.0.0\" , \"apiProfile\" : \"2019-04-01\" , \"parameters\" : { \"resourceGroups\" : { \"type\" : \"array\" , \"metadata\" : { \"description\" : \"Resource groups definition\" } } }, \"resources\" : [ { \"type\" : \"Microsoft.Resources/resourceGroups\" , \"apiVersion\" : \"2019-10-01\" , \"name\" : \"[parameters('resourceGroups')[copyIndex('rgCopy')].name]\" , \"location\" : \"[parameters('resourceGroups')[copyIndex('rgCopy')].location]\" , \"tags\" : \"[parameters('resourceGroups')[copyIndex('rgCopy')].tags]\" , \"properties\" : { }, \"copy\" : { \"name\" : \"rgCopy\" , \"count\" : \"[length(parameters('resourceGroups'))]\" } } ] } function Validate-ResourceGroupDeployment ( $Tags ) { $deploymentParameters = @{ TemplateFile = '.\\ResourceGroups.json' TemplateParameterObject = @{ resourceGroups = @( @{ name = \"rg-tagtest-001\" location = \"eastus2\" tags = $Tags } ) } } return ( Test-AzDeployment @deploymentParameters -Location 'eastus2' ) } What is valid? Here is a list of our test cases in hashtable form. It contains information about whether the tag is required. Also, some valid and invalid values are provided. Try to be tricky as much as you can during providing example wrong values. For example: 29th o February :) empty values uppercase values omitting one of the values (this is difficult to validate, however, don't worry, we'll write an article about advanced Azure Policy development) $properTags = @{ \"Owner\" = 'kamil@automate.guru' \"Environment\" = 'dev' \"BusinessUnit\" = 'itdivision/appsarea/csharpteam' } $tagValuesValidationInput = @( @{ TagName = 'Owner' State = 'Optional' ProperValues = @( 'kamil@automate.guru' 'artur@automationgurus.github.io' ) WrongValues = @( 'kamilautomate.guru' '@automate.guru' '' 'somemone@' 'somemone@.com' 'somemone@.' 'somemone@automate.' ) } @{ TagName = 'Environment' State = 'Required' ProperValues = @( 'dev' 'prod' ) WrongValues = @( '' 'uat' 'DEV' ) } @{ TagName = 'BusinessUnit' State = 'Required' ProperValues = @( 'itdivision/appsarea/csharpteam' ) WrongValues = @( '/appsarea/csharpteam' 'itdivision//csharpteam' 'itdivision/appsarea/' 'itdivision/appsarea/Csharpteam' 'itdivision/Appsarea/csharpteam' 'Itdivision/appsarea/csharpteam' '' '//' '/' ) } @{ TagName = 'ItsmRequestId' State = 'Optional' ProperValues = @( 'sr1990' ) WrongValues = @( '' 'SR1990' '1990' ) } @{ TagName = 'EndDate' State = 'Optional' ProperValues = @( '11062020' '02292024' '06012020' ) WrongValues = @( '11' '' '99999999' '13012020' '11322020' '02292021' ) } ) Resource group creation request must be: allowed when only required tags have proper values and are provided denied if any mandatory tag is not there denied if any mandatory tag has an invalid value allowed when required tags are present and have valid values and optional parameter have a proper value denied when the optional tag has a wrong value Describe \"Tagging Initiative\" { It \"allows RG creation when required tags are provided\" { $validationResult = Validate-ResourceGroupDeployment -Tags $properTags $validationResult | Should -BeNullOrEmpty } $requiredTagNames = ( $tagValuesValidationInput | Where-Object -Property State -EQ 'Required' ). TagName foreach ( $tagName in $requiredTagNames ) { It \"denies RG creation when required tag ' $( $tagName ) ' is missing\" { $tags = $properTags . Clone () $tags . Remove ( $tagName ) $validationResult = Validate-ResourceGroupDeployment -Tags $tags $validationResult | Should -Not -BeNullOrEmpty } } foreach ( $item in $tagValuesValidationInput ) { foreach ( $value in $item . ProperValues ) { It \"allows RG creation when tag ' $( $item . TagName ) ' equals ' $( $value ) '\" { $tags = $properTags . Clone () $tags [ $item . TagName ] = $value $validationResult = Validate-ResourceGroupDeployment -Tags $tags $validationResult | Should -BeNullOrEmpty } } foreach ( $value in $item . WrongValues ) { It \"denies RG creation when tag ' $( $item . TagName ) ' equals ' $( $value ) '\" { $tags = $properTags . Clone () $tags [ $item . TagName ] = $value $validationResult = Validate-ResourceGroupDeployment -Tags $tags $validationResult | Should -Not -BeNullOrEmpty } } } } Let's run Pester tests locally before applying any Azure Policy Continuous Inspection Process Coming soon...","title":"Security And Compliance As A Code Azure Policy TDD"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#security-and-compliance-as-a-code-azure-policy-test-driven-development","text":"According to Wardley Map of Security Skills required by Industry Security and Compliance as a Code is now in Genesis. There can be no more exciting area to dive in. Generally speaking, we're inheriting more and more good practices known for years in the software development world. We've already adopted using source control repository to store our script and declarative definitions of our infrastructures. New tools are providing the capability to do static code analysis for non-classic programming languages. Infrastructure deployment is now a natural part of CD processes. Azure Marketplace is a place where you can find many tools supporting mentioned. What about the future? Let's start by using test-driven development in the process of providing consistency to our Azure subscriptions using Azure Policy .","title":"Security and Compliance as a Code: Azure Policy Test-Driven Development"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#who-is-involved","text":"Pester Powershell module Az.Resources Powershell module Azure Policy service","title":"'Who' is involved?"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#why-develop-automatic-testing-for-azure-policy-its-faster-and-more-reliable","text":"You'll find errors earlier. Therefore, fully implemented, well-working functionality will be delivered faster ! When any new Azure Policy is created, it will be easy to check if current requirements are still adequately implemented. You'll do the next modifications without affright. Finally, it can be proved that tests have been performed . Are you able to test hundreds of tests in a couple of minutes using Azure Portal?","title":"Why develop automatic testing for Azure Policy? It's faster and more reliable."},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#test-driven-development-process","text":"Implement tests (only this is covered in this article). Implement functionality.","title":"Test-Driven Development Process"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#show-me-that-in-action","text":"As always, we want to explain it to you better using a practical use case. So we want to govern Resource Group tagging. Let's say that there are some requirements defined.","title":"Show me that in action."},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#more-details","text":"Tag Name Required? Description Example Value Owner Yes Owner's email address kamil@automate.guru artur@automationgurus.github.io Environment Yes Can be only dev or prod dev, prod BusinessUnit Yes Division/Area/Team, all parts must be filled n itdivision/appsarea/csharpteam EndDate No Decommissioning date (DDMMYYYY) 29022024 ItsmRequestId No Service Request's ID sr0001, sr1990 A table and flow chart should be enough for you to understand what we want to achieve. Additional information: tag values must be lowercase, only allowed domains in the email address are 'automate.guru' and 'automationgurus.github.io'.","title":"More details"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#test-procedure","text":"Create hashtable with some valid and invalid values, also put information if a particular tag presence is required. Assign initiative containing all policies enforcing business requirements at subscription scope. Wait 30 minutes to be sure that all assignments are applied. Use Test-Deployment Powershell to validate deployments in scenarios based on input from pt. 1. Checks results.","title":"Test procedure"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#azure-resource-group-deployment-validation-function","text":"We need a function that tries to create RG with tags provided by parameters. This function uses a universal ARM Template for deploying multiple RG. { \"$schema\" : \"https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\" , \"contentVersion\" : \"1.0.0.0\" , \"apiProfile\" : \"2019-04-01\" , \"parameters\" : { \"resourceGroups\" : { \"type\" : \"array\" , \"metadata\" : { \"description\" : \"Resource groups definition\" } } }, \"resources\" : [ { \"type\" : \"Microsoft.Resources/resourceGroups\" , \"apiVersion\" : \"2019-10-01\" , \"name\" : \"[parameters('resourceGroups')[copyIndex('rgCopy')].name]\" , \"location\" : \"[parameters('resourceGroups')[copyIndex('rgCopy')].location]\" , \"tags\" : \"[parameters('resourceGroups')[copyIndex('rgCopy')].tags]\" , \"properties\" : { }, \"copy\" : { \"name\" : \"rgCopy\" , \"count\" : \"[length(parameters('resourceGroups'))]\" } } ] } function Validate-ResourceGroupDeployment ( $Tags ) { $deploymentParameters = @{ TemplateFile = '.\\ResourceGroups.json' TemplateParameterObject = @{ resourceGroups = @( @{ name = \"rg-tagtest-001\" location = \"eastus2\" tags = $Tags } ) } } return ( Test-AzDeployment @deploymentParameters -Location 'eastus2' ) }","title":"Azure Resource Group deployment validation function"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#what-is-valid","text":"Here is a list of our test cases in hashtable form. It contains information about whether the tag is required. Also, some valid and invalid values are provided. Try to be tricky as much as you can during providing example wrong values. For example: 29th o February :) empty values uppercase values omitting one of the values (this is difficult to validate, however, don't worry, we'll write an article about advanced Azure Policy development) $properTags = @{ \"Owner\" = 'kamil@automate.guru' \"Environment\" = 'dev' \"BusinessUnit\" = 'itdivision/appsarea/csharpteam' } $tagValuesValidationInput = @( @{ TagName = 'Owner' State = 'Optional' ProperValues = @( 'kamil@automate.guru' 'artur@automationgurus.github.io' ) WrongValues = @( 'kamilautomate.guru' '@automate.guru' '' 'somemone@' 'somemone@.com' 'somemone@.' 'somemone@automate.' ) } @{ TagName = 'Environment' State = 'Required' ProperValues = @( 'dev' 'prod' ) WrongValues = @( '' 'uat' 'DEV' ) } @{ TagName = 'BusinessUnit' State = 'Required' ProperValues = @( 'itdivision/appsarea/csharpteam' ) WrongValues = @( '/appsarea/csharpteam' 'itdivision//csharpteam' 'itdivision/appsarea/' 'itdivision/appsarea/Csharpteam' 'itdivision/Appsarea/csharpteam' 'Itdivision/appsarea/csharpteam' '' '//' '/' ) } @{ TagName = 'ItsmRequestId' State = 'Optional' ProperValues = @( 'sr1990' ) WrongValues = @( '' 'SR1990' '1990' ) } @{ TagName = 'EndDate' State = 'Optional' ProperValues = @( '11062020' '02292024' '06012020' ) WrongValues = @( '11' '' '99999999' '13012020' '11322020' '02292021' ) } )","title":"What is valid?"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#resource-group-creation-request-must-be","text":"allowed when only required tags have proper values and are provided denied if any mandatory tag is not there denied if any mandatory tag has an invalid value allowed when required tags are present and have valid values and optional parameter have a proper value denied when the optional tag has a wrong value Describe \"Tagging Initiative\" { It \"allows RG creation when required tags are provided\" { $validationResult = Validate-ResourceGroupDeployment -Tags $properTags $validationResult | Should -BeNullOrEmpty } $requiredTagNames = ( $tagValuesValidationInput | Where-Object -Property State -EQ 'Required' ). TagName foreach ( $tagName in $requiredTagNames ) { It \"denies RG creation when required tag ' $( $tagName ) ' is missing\" { $tags = $properTags . Clone () $tags . Remove ( $tagName ) $validationResult = Validate-ResourceGroupDeployment -Tags $tags $validationResult | Should -Not -BeNullOrEmpty } } foreach ( $item in $tagValuesValidationInput ) { foreach ( $value in $item . ProperValues ) { It \"allows RG creation when tag ' $( $item . TagName ) ' equals ' $( $value ) '\" { $tags = $properTags . Clone () $tags [ $item . TagName ] = $value $validationResult = Validate-ResourceGroupDeployment -Tags $tags $validationResult | Should -BeNullOrEmpty } } foreach ( $value in $item . WrongValues ) { It \"denies RG creation when tag ' $( $item . TagName ) ' equals ' $( $value ) '\" { $tags = $properTags . Clone () $tags [ $item . TagName ] = $value $validationResult = Validate-ResourceGroupDeployment -Tags $tags $validationResult | Should -Not -BeNullOrEmpty } } } }","title":"Resource group creation request must be:"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#lets-run-pester-tests-locally-before-applying-any-azure-policy","text":"","title":"Let's run Pester tests locally before applying any Azure Policy"},{"location":"security-and-compliance-as-a-code-azure-policy-tdd/#continuous-inspection-process","text":"Coming soon...","title":"Continuous Inspection Process"},{"location":"worth-to-check/","text":"Udefull links","title":"Worth to check"},{"location":"worth-to-check/#udefull-links","text":"","title":"Udefull links"}]}